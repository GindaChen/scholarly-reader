<h1>Untitled Paper</h1>

<p class="authors"></p>

<h2>Abstract</h2>
<p>In this work we explore recent advances in Recurrent Neural Networks for large scale Language Modeling, a task central to language understanding. We extend current models to deal with two key challenges present in this task: corpora and vocabulary sizes, and complex, long term structure of language. We perform an exhaustive study on techniques such as character Convolutional Neural Networks or Long-Short Term Memory, on the One Billion Word Benchmark. Our best single model significantly improves state-of-the-art perplexity from 51.3 down to 30.0 (whilst reducing the number of parameters by a factor of 20), while an ensemble of models sets a new record by improving perplexity from 41.0 down to 23.7. We also release these models for the NLP and ML community to study and improve upon.</p>

<hr>

<h2>1. Introduction</h2>

<span id="intro" class="label-anchor"></span>

<p>Language Modeling (LM) is a task central to Natural Language Processing (NLP) and Language Understanding. Models which can accurately place distributions over sentences not only encode complexities of language such as grammatical structure, but also distill a fair amount of information about the knowledge that a corpora may contain. Indeed, models that are able to assign a low probability to sentences that are grammatically correct but unlikely may help other tasks in fundamental language understanding like question answering, machine translation, or text summarization.</p>

<p>LMs have played a key role in traditional NLP tasks such as speech recognition <sup class="ref-badge" data-ref="30" data-title="Khudanpur, Sanjeev.">30</sup><sup class="ref-badge" data-ref="2" data-title="Deep neural network language models.">2</sup>, machine translation <sup class="ref-badge" data-ref="39" data-title="Large, pruned or continuous space language models on a gpu for">39</sup><sup class="ref-badge" data-ref="47" data-title="Decoding with large-scale neural language models improves">47</sup>, or text summarization <sup class="ref-badge" data-ref="36" data-title="A neural attention model for abstractive sentence summarization." data-arxiv-id="1509.00685">36</sup><sup class="ref-badge" data-ref="10" data-title="Vinyals, Oriol.">10</sup>. Often (although not always), training better language models improves the underlying metrics of the downstream task (such as word error rate for speech recognition, or BLEU score for translation), which makes the task of training better LMs valuable by itself.</p>

<p>Further, when trained on vast amounts of data, language models compactly extract knowledge encoded in the training data. For example, when trained on movie subtitles <sup class="ref-badge" data-ref="40" data-title="and Pineau, Joelle.">40</sup><sup class="ref-badge" data-ref="49" data-title="A neural conversational model." data-arxiv-id="1506.05869">49</sup>, these language models are able to generate basic answers to questions about object colors, facts about people, etc. Lastly, recently proposed sequence-to-sequence models employ conditional language models <sup class="ref-badge" data-ref="29" data-title="Context dependent recurrent neural network language model.">29</sup> as their key component to solve diverse tasks like machine translation <sup class="ref-badge" data-ref="46" data-title="Sequence to sequence learning with neural networks.">46</sup><sup class="ref-badge" data-ref="8" data-title="Bougares, Fethi, Schwenk, Holger, and Bengio, Yoshua." data-arxiv-id="1406.1078">8</sup><sup class="ref-badge" data-ref="20" data-title="A convolutional neural network for modelling sentences." data-arxiv-id="1404.2188">20</sup> or video generation <sup class="ref-badge" data-ref="43" data-title="Unsupervised learning of video representations using lstms." data-arxiv-id="1502.04681">43</sup>.</p>

<figure id="main_figure">
<img src="./figures/Figure1_LMPAPER.png" alt="A high-level diagram of the models presented in this paper. (a) is a standard LSTM LM. (b) represents an LM where both input and Softmax embeddings have been replaced by a character CNN. In (c) we replace the Softmax by a next character prediction LSTM network." loading="lazy" style="max-width:100%">
<figcaption><strong>Figure 1.</strong> A high-level diagram of the models presented in this paper. (a) is a standard LSTM LM. (b) represents an LM where both input and Softmax embeddings have been replaced by a character CNN. In (c) we replace the Softmax by a next character prediction LSTM network.</figcaption>
</figure>

<p>Deep Learning and Recurrent Neural Networks (RNNs) have fueled language modeling research in the past years as it allowed researchers to explore many tasks for which the strong conditional independence assumptions are unrealistic. Despite the fact that simpler models, such as N-grams, only use a short history of previous words to predict the next word, they are still a key component to high quality, low perplexity LMs. Indeed, most recent work on large scale LM has shown that RNNs are great in combination with N-grams, as they may have different strengths that complement N-gram models, but worse when considered in isolation <sup class="ref-badge" data-ref="31" data-title="Cernock\`y, Jan.">31</sup><sup class="ref-badge" data-ref="28" data-title="Statistical language models based on neural networks.">28</sup><sup class="ref-badge" data-ref="7" data-title="Koehn, Phillipp, and Robinson, Tony." data-arxiv-id="1312.3005">7</sup><sup class="ref-badge" data-ref="52" data-title="Scaling recurrent neural network language models.">52</sup><sup class="ref-badge" data-ref="17" data-title="Dubey, Pradeep.">17</sup><sup class="ref-badge" data-ref="41" data-title="Sparse non-negative matrix language modeling for skip-grams.">41</sup>.</p>

<p>We believe that, despite much work being devoted to small data sets like the Penn Tree Bank (PTB) <sup class="ref-badge" data-ref="27" data-title="Building a large annotated corpus of english: The penn treebank.">27</sup>, research on larger tasks is very relevant as overfitting is not the main limitation in current language modeling, but is the main characteristic of the PTB task. Results on larger corpora usually show better what matters as many ideas work well on small data sets but fail to improve on larger data sets. Further, given current hardware trends and vast amounts of text available on the Web, it is much more straightforward to tackle large scale modeling than it used to be. Thus, we hope that our work will help and motivate researchers to work on traditional LM beyond PTB -- for this purpose, we will open-source our models and training recipes.</p>

<p>We focused on a well known, large scale LM benchmark: the One Billion Word Benchmark data set <sup class="ref-badge" data-ref="7" data-title="Koehn, Phillipp, and Robinson, Tony." data-arxiv-id="1312.3005">7</sup>. This data set is much larger than PTB (one thousand fold,  800k word vocabulary and  1B words training data) and far more challenging. Similar to Imagenet <sup class="ref-badge" data-ref="9" data-title="Imagenet: A large-scale hierarchical image database.">9</sup>, which helped advance computer vision, we believe that releasing and working on large data sets and models with clear benchmarks will help advance Language Modeling.</p>

<p>The contributions of our work are as follows:</p>

- We explored, extended and tried to unify some of the current research on large scale LM.
- Specifically, we designed a Softmax loss which is based on character level CNNs, is efficient to train, and is as precise as a full Softmax which has orders of magnitude more parameters.
- Our study yielded significant improvements to the state-of-the-art on a well known, large scale LM task: from 51.3 down to 30.0 perplexity for single models whilst reducing the number of parameters by a factor of 20.
- We show that an ensemble of a number of different models can bring down perplexity on this task to 23.7, a large improvement compared to current state-of-art.
- We share the model and recipes in order to help and motivate further research in this area.

<p>In Section~<a href="#relwork" class="cross-ref">Section 2</a> we review important concepts and previous work on language modeling. Section~<a href="#model" class="cross-ref">Section 6</a> presents our contributions to the field of neural language modeling, emphasizing large scale recurrent neural network training. Sections <a href="#exps" class="cross-ref">Section 10</a> and <a href="#results" class="cross-ref">Section 13</a> aim at exhaustively describing our experience and understanding throughout the project, as well as emplacing our work relative to other known approaches.</p>

<hr>

<h2>2. Related Work</h2>

<span id="relwork" class="label-anchor"></span>

<p>In this section we describe previous work relevant to the approaches discussed in this paper. A more detailed discussion on language modeling research is provided in <sup class="ref-badge" data-ref="28" data-title="Statistical language models based on neural networks.">28</sup>.</p>

### Language Models
<span id="lms" class="label-anchor"></span>

<p>Language Modeling (LM) has been a central task in NLP. The goal of LM is to learn a probability distribution over sequences of symbols pertaining to a language. Much work has been done on both parametric (e.g., log-linear models) and non-parametric approaches (e.g., count-based LMs). Count-based approaches (based on statistics of N-grams) typically add smoothing which account for unseen (yet possible) sequences, and have been quite successful. To this extent, Kneser-Ney smoothed 5-gram models <sup class="ref-badge" data-ref="22" data-title="Improved backing-off for m-gram language modeling.">22</sup> are a fairly strong baseline which, for large amounts of training data, have challenged other parametric approaches based on Neural Networks <sup class="ref-badge" data-ref="6" data-title="Fr\'ederic, and Gauvain, Jean-Luc.">6</sup>.</p>

<p>Most of our work is based on Recurrent Neural Networks (RNN) models which retain long term dependencies. To this extent, we used the Long-Short Term Memory model <sup class="ref-badge" data-ref="16" data-title="Long short-term memory.">16</sup> which uses a gating mechanism <sup class="ref-badge" data-ref="11" data-title="Learning to forget: Continual prediction with lstm.">11</sup> to ensure proper propagation of information through many time steps. Much work has been done on small and large scale RNN-based LMs <sup class="ref-badge" data-ref="30" data-title="Khudanpur, Sanjeev.">30</sup><sup class="ref-badge" data-ref="28" data-title="Statistical language models based on neural networks.">28</sup><sup class="ref-badge" data-ref="7" data-title="Koehn, Phillipp, and Robinson, Tony." data-arxiv-id="1312.3005">7</sup><sup class="ref-badge" data-ref="53" data-title="Recurrent neural network regularization." data-arxiv-id="1409.2329">53</sup><sup class="ref-badge" data-ref="52" data-title="Scaling recurrent neural network language models.">52</sup><sup class="ref-badge" data-ref="17" data-title="Dubey, Pradeep.">17</sup><sup class="ref-badge" data-ref="50" data-title="Larger-context language modelling." data-arxiv-id="1511.03729">50</sup><sup class="ref-badge" data-ref="18" data-title="Document context language models." data-arxiv-id="1511.03962">18</sup>. The architectures that we considered in this paper are represented in Figure~<a href="#main_figure" class="cross-ref">Fig. 1</a>.</p>

<p>In our work, we train models on the popular One Billion Word Benchmark, which can be considered to be a medium-sized data set for count-based LMs but a very large data set for NN-based LMs. This regime is most interesting to us as we believe learning a very good model of human language is a complex task which will require large models, and thus large amounts of data. Further advances in data availability and computational resources helped our study. We argue this leap in scale enabled tremendous advances in deep learning. A clear example found in computer vision is Imagenet <sup class="ref-badge" data-ref="9" data-title="Imagenet: A large-scale hierarchical image database.">9</sup>, which enabled learning complex vision models from large amounts of data <sup class="ref-badge" data-ref="23" data-title="Imagenet classification with deep convolutional neural networks.">23</sup>.</p>

<p>A crucial aspect which we discuss in detail in later sections is the size of our models. Despite the large number of parameters, we try to minimize computation as much as possible by adopting a strategy proposed in <sup class="ref-badge" data-ref="37" data-title="Long short-term memory recurrent neural network architectures for">37</sup> of projecting a relatively big recurrent state space down so that the matrices involved remain relatively small, yet the model has large memory capacity.</p>

### Convolutional Embedding Models
<span id="input_cnn" class="label-anchor"></span>

<p>There is an increased interest in incorporating character-level inputs to build word embeddings for various NLP problems, including part-of-speech tagging, parsing and language modeling <sup class="ref-badge" data-ref="25" data-title="Ram\'on Fernandez, Amir, Silvio, Dyer, Chris, Black, Alan W, and Trancoso," data-arxiv-id="1508.02096">25</sup><sup class="ref-badge" data-ref="21" data-title="Character-aware neural language models." data-arxiv-id="1508.06615">21</sup><sup class="ref-badge" data-ref="3" data-title="Improved transition-based parsing by modeling characters instead of" data-arxiv-id="1508.00657">3</sup>. The additional character information has been shown useful on relatively small benchmark data sets.</p>

<p>The approach proposed in <sup class="ref-badge" data-ref="25" data-title="Ram\'on Fernandez, Amir, Silvio, Dyer, Chris, Black, Alan W, and Trancoso," data-arxiv-id="1508.02096">25</sup> builds word embeddings using bidirectional LSTMs <sup class="ref-badge" data-ref="38" data-title="Bidirectional recurrent neural networks.">38</sup><sup class="ref-badge" data-ref="14" data-title="Framewise phoneme classification with bidirectional lstm and other">14</sup> over the characters. The recurrent networks process sequences of characters from both sides and their final state vectors are concatenated. The resulting representation is then fed to a Neural Network. This model achieved very good results on a part-of-speech tagging task.</p>

<p>In <sup class="ref-badge" data-ref="21" data-title="Character-aware neural language models." data-arxiv-id="1508.06615">21</sup>, the words characters are processed by a 1-d CNN <sup class="ref-badge" data-ref="24" data-title="and Jackel, Lawrence D.">24</sup> with max-pooling across the sequence for each convolutional feature. The resulting features are fed to a 2-layer highway network <sup class="ref-badge" data-ref="44" data-title="Training very deep networks.">44</sup>, which allows the embedding to learn semantic representations. The model was evaluated on small-scale language modeling experiments for various languages and matched the best results on the PTB data set despite having 60\% fewer parameters.</p>

### Softmax Over Large Vocabularies
<span id="large_soft" class="label-anchor"></span>

<p>Assigning probability distributions over large vocabularies is computationally challenging. For modeling language, maximizing log-likelihood of a given word sequence leads to optimizing cross-entropy between the target probability distribution (e.g., the target word we should be predicting), and our model predictions <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">p</span></span></span></span></span>. Generally, predictions come from a linear layer followed by a Softmax non-linearity: <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy="false">(</mo><mi>w</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>z</mi><mi>w</mi></msub><mo stretchy="false">)</mo></mrow><mrow><msub><mo>∑</mo><mrow><msup><mi>w</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo>∈</mo><mi>V</mi></mrow></msub><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>z</mi><msup><mi>w</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></msub><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">p(w) = \frac{\exp(z_w)}{\sum_{w&#x27; \in V} \exp(z_{w&#x27;})}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.5996em;vertical-align:-0.5896em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.01em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mop mtight"><span class="mop op-symbol small-op mtight" style="position:relative;top:0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2854em;"><span style="top:-2.2854em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.6068em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8496em;"><span style="top:-2.8496em;margin-right:0.1em;"><span class="pstrut" style="height:2.5556em;"></span><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span><span class="mrel mtight">∈</span><span class="mord mathnormal mtight" style="margin-right:0.22222em;">V</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3494em;"><span></span></span></span></span></span></span><span class="mspace mtight" style="margin-right:0.1952em;"></span><span class="mop mtight"><span class="mtight">e</span><span class="mtight">x</span><span class="mtight">p</span></span><span class="mopen mtight">(</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3448em;margin-left:-0.044em;margin-right:0.0714em;"><span class="pstrut" style="height:2.6068em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8496em;"><span style="top:-2.8496em;margin-right:0.1em;"><span class="pstrut" style="height:2.5556em;"></span><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.262em;"><span></span></span></span></span></span></span><span class="mclose mtight">)</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.485em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mop mtight"><span class="mtight">e</span><span class="mtight">x</span><span class="mtight">p</span></span><span class="mopen mtight">(</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1645em;"><span style="top:-2.357em;margin-left:-0.044em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.5896em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span> where <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mi>w</mi></msub></mrow><annotation encoding="application/x-tex">z_w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.044em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span> is the logit corresponding to a word <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span></span></span></span></span>. The logit is generally computed as an inner product <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mi>w</mi></msub><mo>=</mo><msup><mi>h</mi><mi>T</mi></msup><msub><mi>e</mi><mi>w</mi></msub></mrow><annotation encoding="application/x-tex">z_w = h^Te_w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.044em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.9913em;vertical-align:-0.15em;"></span><span class="mord"><span class="var" data-var="h" data-desc="Number of parallel attention heads (h = 8)" style="--var-color: #ff7b72"><span class="mord mathnormal">h</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span> where <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi></mrow><annotation encoding="application/x-tex">h</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="var" data-var="h" data-desc="Number of parallel attention heads (h = 8)" style="--var-color: #ff7b72"><span class="mord mathnormal">h</span></span></span></span></span></span> is a context vector and <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>e</mi><mi>w</mi></msub></mrow><annotation encoding="application/x-tex">e_w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span> is a ``word embedding'' for <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span></span></span></span></span>.</p>

<p>The main challenge when <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∣</mi><mi>V</mi><mi mathvariant="normal">∣</mi></mrow><annotation encoding="application/x-tex">|V|</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">∣</span><span class="var" data-var="V" data-desc="Value matrix — packed values (shape: seq_len × d_v)" style="--var-color: #7ee787"><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span><span class="mord">∣</span></span></span></span></span> is very large (in the order of one million in this paper) is the fact that computing all inner products between <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi></mrow><annotation encoding="application/x-tex">h</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="var" data-var="h" data-desc="Number of parallel attention heads (h = 8)" style="--var-color: #ff7b72"><span class="mord mathnormal">h</span></span></span></span></span></span> and all embeddings becomes prohibitively slow during training (even when exploiting matrix-matrix multiplications and modern GPUs). Several approaches have been proposed to cope with the scaling issue: importance sampling <sup class="ref-badge" data-ref="5" data-title="Quick training of probabilistic neural nets by importance sampling.">5</sup><sup class="ref-badge" data-ref="4" data-title="Adaptive importance sampling to accelerate training of a neural">4</sup>, Noise Contrastive Estimation (NCE) <sup class="ref-badge" data-ref="15" data-title="Noise-contrastive estimation: A new estimation principle for">15</sup><sup class="ref-badge" data-ref="33" data-title="Learning word embeddings efficiently with noise-contrastive">33</sup>, self normalizing partition functions <sup class="ref-badge" data-ref="48" data-title="Efficient exact gradient update for training deep networks with very">48</sup> or Hierarchical Softmax <sup class="ref-badge" data-ref="34" data-title="Hierarchical probabilistic neural network language model.">34</sup><sup class="ref-badge" data-ref="32" data-title="A scalable hierarchical distributed language model.">32</sup> -- they all offer good solutions to this problem. We found importance sampling to be quite effective on this task, and explain the connection between it and NCE in the following section, as they are closely related.</p>

<hr>

<h2>3. Language Modeling Improvements</h2>

<span id="model" class="label-anchor"></span>

<p>Recurrent Neural Networks based LMs employ the chain rule to model joint probabilities over word sequences:</p>

<div class="math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>p</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>w</mi><mi>N</mi></msub><mo stretchy="false">)</mo><mo>=</mo><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mi>p</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mi>i</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>w</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>w</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p(w_1,\ldots,w_N) = \prod_{i=1}^N p(w_i | w_1, \ldots, w_{i-1})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:3.106em;vertical-align:-1.2777em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283em;"><span style="top:-1.8723em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∏</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2777em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></div>

<p>where the context of all previous words is encoded with an LSTM, and the probability over words uses a Softmax (see Figure~<a href="#main_figure" class="cross-ref">Fig. 1</a>(a)).</p>

### Relationship between Noise Contrastive Estimation and Importance Sampling
<span id="is_soft" class="label-anchor"></span>

<p>As discussed in Section~<a href="#large_soft" class="cross-ref">Section 5</a>, a large scale Softmax is necessary for training good LMs because of the vocabulary size. A Hierarchical Softmax <sup class="ref-badge" data-ref="32" data-title="A scalable hierarchical distributed language model.">32</sup> employs a tree in which the probability distribution over words is decomposed into a product of two probabilities for each word, greatly reducing training and inference time as only the path specified by the hierarchy needs to be computed and updated. Choosing a good hierarchy is important for obtaining good results and we did not explore this approach further for this paper as sampling methods worked well for our setup.</p>

<p>Sampling approaches are only useful during training, as they propose an approximation to the loss which is cheap to compute (also in a distributed setting) -- however, at inference time one still has to compute the normalization term over all words. Noise Contrastive Estimation (NCE) proposes to consider a surrogate binary classification task in which a classifier is trained to discriminate between true data, or samples coming from some arbitrary distribution. If both the noise and data distributions were known, the optimal classifier would be:</p>

<div class="math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>p</mi><mo stretchy="false">(</mo><mi>Y</mi><mo>=</mo><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi><mi mathvariant="normal">∣</mi><mi>w</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><msub><mi>p</mi><mi>d</mi></msub><mo stretchy="false">(</mo><mi>w</mi><mo stretchy="false">)</mo></mrow><mrow><msub><mi>p</mi><mi>d</mi></msub><mo stretchy="false">(</mo><mi>w</mi><mo stretchy="false">)</mo><mo>+</mo><mi>k</mi><msub><mi>p</mi><mi>n</mi></msub><mo stretchy="false">(</mo><mi>w</mi><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">p(Y=true | w) = \frac{p_d(w)}{p_d(w) + k p_n(w)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">t</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">u</span><span class="mord mathnormal">e</span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.363em;vertical-align:-0.936em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mclose">)</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></div>

<p>where <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi></mrow><annotation encoding="application/x-tex">Y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span></span></span></span></span> is the binary random variable indicating whether <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span></span></span></span></span> comes from the true data distribution, <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span></span></span></span></span> is the number of negative samples per positive word, and <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mi>d</mi></msub></mrow><annotation encoding="application/x-tex">p_d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span> and <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">p_n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span> are the data and noise distribution respectively (we dropped any dependency on previous words for notational simplicity).</p>

<p>It is easy to show that if we train a logistic classifier <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><mi>Y</mi><mo>=</mo><mi>t</mi><mi>r</mi><mi>u</mi><mi>e</mi><mi mathvariant="normal">∣</mi><mi>w</mi><mo stretchy="false">)</mo><mo>=</mo><mi>σ</mi><mo stretchy="false">(</mo><msub><mi>s</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><mi>w</mi><mo separator="true">,</mo><mi>h</mi><mo stretchy="false">)</mo><mo>−</mo><mi>log</mi><mo>⁡</mo><mi>k</mi><msub><mi>p</mi><mi>n</mi></msub><mo stretchy="false">(</mo><mi>w</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p_\theta(Y=true|w) = \sigma(s_\theta(w,h)-\log k p_n(w))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">t</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">u</span><span class="mord mathnormal">e</span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="var" data-var="h" data-desc="Number of parallel attention heads (h = 8)" style="--var-color: #ff7b72"><span class="mord mathnormal">h</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mclose">))</span></span></span></span></span> where <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>σ</mi></mrow><annotation encoding="application/x-tex">\sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span></span></span></span></span> is the logistic function, then, <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>p</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">(</mo><mi>w</mi><mo stretchy="false">)</mo><mo>=</mo><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><msub><mi>s</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><mi>w</mi><mo separator="true">,</mo><mi>h</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p&#x27;(w)=softmax(s_\theta(w,h))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0019em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">so</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mord mathnormal">t</span><span class="mord mathnormal">ma</span><span class="var" data-var="x" data-desc="Input tensor" style="--var-color: #98c379"><span class="mord mathnormal">x</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="var" data-var="h" data-desc="Number of parallel attention heads (h = 8)" style="--var-color: #ff7b72"><span class="mord mathnormal">h</span></span><span class="mclose">))</span></span></span></span></span> is a good approximation of <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mi>d</mi></msub><mo stretchy="false">(</mo><mi>w</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p_d(w)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mclose">)</span></span></span></span></span> (<span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>s</mi><mi>θ</mi></msub></mrow><annotation encoding="application/x-tex">s_\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span> is a logit which e.g. an LSTM LM computes).</p>

<p>The other technique, which is based on importance sampling (IS), proposes to directly approximate the partition function (which comprises a sum over all words) with an estimate of it through importance sampling. Though the methods look superficially similar, we will derive a similar surrogate classification task akin to NCE which arrives at IS, showing a strong connection between the two.</p>

<p>Suppose that, instead of having a binary task to decide if a word comes from the data or from the noise distribution, we want to identify the words coming from the true data distribution in a set <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi><mo>=</mo><mo stretchy="false">{</mo><msub><mi>w</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>w</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">W=\{w_1,\ldots,w_{k+1}\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="var" data-var="W" data-desc="Weight matrix" style="--var-color: #a5d6ff"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">{</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span><span class="mclose">}</span></span></span></span></span>, comprised of <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span></span></span></span></span> noise samples and one data distribution sample. Thus, we can train a multiclass loss over a multinomial random variable <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi></mrow><annotation encoding="application/x-tex">Y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span></span></span></span></span> which maximizes <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>log</mi><mo>⁡</mo><mi>p</mi><mo stretchy="false">(</mo><mi>Y</mi><mo>=</mo><mn>1</mn><mi mathvariant="normal">∣</mi><mi>W</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\log p(Y = 1 | W)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1∣</span><span class="var" data-var="W" data-desc="Weight matrix" style="--var-color: #a5d6ff"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span></span><span class="mclose">)</span></span></span></span></span>, assuming w.l.o.g. that <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mn>1</mn></msub><mo>∈</mo><mi>W</mi></mrow><annotation encoding="application/x-tex">w_1 \in W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6891em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="var" data-var="W" data-desc="Weight matrix" style="--var-color: #a5d6ff"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span></span></span></span></span></span> is always the word coming from true data. By Bayes rule, and ignoring terms that are constant with respect to <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi></mrow><annotation encoding="application/x-tex">Y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span></span></span></span></span>, we can write:</p>

<div class="math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>p</mi><mo stretchy="false">(</mo><mi>Y</mi><mo>=</mo><mi>k</mi><mi mathvariant="normal">∣</mi><mi>W</mi><mo stretchy="false">)</mo><msub><mo>∝</mo><mi>Y</mi></msub><mfrac><mrow><msub><mi>p</mi><mi>d</mi></msub><mo stretchy="false">(</mo><msub><mi>w</mi><mi>k</mi></msub><mo stretchy="false">)</mo></mrow><mrow><msub><mi>p</mi><mi>n</mi></msub><mo stretchy="false">(</mo><msub><mi>w</mi><mi>k</mi></msub><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">p(Y=k | W) \propto_Y \frac{p_d(w_k)}{p_n(w_k)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mord">∣</span><span class="var" data-var="W" data-desc="Weight matrix" style="--var-color: #a5d6ff"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel"><span class="mrel">∝</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.22222em;">Y</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.363em;vertical-align:-0.936em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></div>

<p>and, following a similar argument than for NCE, if we define <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy="false">(</mo><mi>Y</mi><mo>=</mo><mi>k</mi><mi mathvariant="normal">∣</mi><mi>W</mi><mo stretchy="false">)</mo><mo>=</mo><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><msub><mi>s</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><msub><mi>w</mi><mi>k</mi></msub><mo stretchy="false">)</mo><mo>−</mo><mi>log</mi><mo>⁡</mo><msub><mi>p</mi><mi>n</mi></msub><mo stretchy="false">(</mo><msub><mi>w</mi><mi>k</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p(Y=k | W)=softmax(s_\theta(w_k)-\log  p_n(w_k))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mord">∣</span><span class="var" data-var="W" data-desc="Weight matrix" style="--var-color: #a5d6ff"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">so</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mord mathnormal">t</span><span class="mord mathnormal">ma</span><span class="var" data-var="x" data-desc="Input tensor" style="--var-color: #98c379"><span class="mord mathnormal">x</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">))</span></span></span></span></span> then <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>p</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">(</mo><mi>w</mi><mo stretchy="false">)</mo><mo>=</mo><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><msub><mi>s</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><mi>w</mi><mo separator="true">,</mo><mi>h</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p&#x27;(w)=softmax(s_\theta(w,h))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0019em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">so</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mord mathnormal">t</span><span class="mord mathnormal">ma</span><span class="var" data-var="x" data-desc="Input tensor" style="--var-color: #98c379"><span class="mord mathnormal">x</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="var" data-var="h" data-desc="Number of parallel attention heads (h = 8)" style="--var-color: #ff7b72"><span class="mord mathnormal">h</span></span><span class="mclose">))</span></span></span></span></span> is a good approximation of <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mi>d</mi></msub><mo stretchy="false">(</mo><mi>w</mi><mi>o</mi><mi>r</mi><mi>d</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p_d(word)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mord mathnormal" style="margin-right:0.02778em;">or</span><span class="mord mathnormal">d</span><span class="mclose">)</span></span></span></span></span>. Note that the only difference between NCE and IS is that, in NCE, we define a binary classification task between true or noise words with a logistic loss, whereas in IS we define a multiclass classification problem with a Softmax and cross entropy loss. We hope that our derivation helps clarify the similarities and differences between the two. In particular, we observe that IS, as it optimizes a multiclass classification task (in contrast to solving a binary task), may be a better choice. Indeed, the updates to the logits with IS are tied whereas in NCE they are independent.</p>

### CNN Softmax
<span id="cnn_softmax" class="label-anchor"></span>

<p>The character-level features allow for a smoother and compact parametrization of the word embeddings. Recent efforts on small scale language modeling have used CNN character embeddings for the input embeddings <sup class="ref-badge" data-ref="21" data-title="Character-aware neural language models." data-arxiv-id="1508.06615">21</sup>. Although not as straightforward, we propose an extension to this idea to also reduce the number of parameters of the Softmax layer. Recall from Section~<a href="#large_soft" class="cross-ref">Section 5</a> that the Softmax computes a logit as <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mi>w</mi></msub><mo>=</mo><msup><mi>h</mi><mi>T</mi></msup><msub><mi>e</mi><mi>w</mi></msub></mrow><annotation encoding="application/x-tex">z_w=h^Te_w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.044em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.9913em;vertical-align:-0.15em;"></span><span class="mord"><span class="var" data-var="h" data-desc="Number of parallel attention heads (h = 8)" style="--var-color: #ff7b72"><span class="mord mathnormal">h</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span> where <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi></mrow><annotation encoding="application/x-tex">h</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="var" data-var="h" data-desc="Number of parallel attention heads (h = 8)" style="--var-color: #ff7b72"><span class="mord mathnormal">h</span></span></span></span></span></span> is a context vector and <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>e</mi><mi>w</mi></msub></mrow><annotation encoding="application/x-tex">e_w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span> the word embedding. Instead of building a matrix of <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∣</mi><mi>V</mi><mi mathvariant="normal">∣</mi><mo>×</mo><mi mathvariant="normal">∣</mi><mi>h</mi><mi mathvariant="normal">∣</mi></mrow><annotation encoding="application/x-tex">|V|\times |h|</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">∣</span><span class="var" data-var="V" data-desc="Value matrix — packed values (shape: seq_len × d_v)" style="--var-color: #7ee787"><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span><span class="mord">∣</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">∣</span><span class="var" data-var="h" data-desc="Number of parallel attention heads (h = 8)" style="--var-color: #ff7b72"><span class="mord mathnormal">h</span></span><span class="mord">∣</span></span></span></span></span> (whose rows correspond to <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>e</mi><mi>w</mi></msub></mrow><annotation encoding="application/x-tex">e_w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span>), we produce <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>e</mi><mi>w</mi></msub></mrow><annotation encoding="application/x-tex">e_w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span> with a CNN over the characters of <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span></span></span></span></span> as <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>e</mi><mi>w</mi></msub><mo>=</mo><mi>C</mi><mi>N</mi><mi>N</mi><mo stretchy="false">(</mo><mi>c</mi><mi>h</mi><mi>a</mi><mi>r</mi><msub><mi>s</mi><mi>w</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">e_w = CNN(chars_w)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mopen">(</span><span class="mord mathnormal">c</span><span class="mord mathnormal">ha</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span> -- we call this a CNN Softmax. We used the same network architecture to dynamically generate the Softmax word embeddings without sharing the parameters with the input word-embedding sub-network. For inference, the vectors <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>e</mi><mi>w</mi></msub></mrow><annotation encoding="application/x-tex">e_w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span> can be precomputed, so there is no computational complexity increase w.r.t. the regular Softmax.</p>

<p>We note that, when using an importance sampling loss such as the one described in Section~<a href="#is_soft" class="cross-ref">Section 7</a>, only a few logits have non-zero gradient (those corresponding to the true and sampled words). With a Softmax where <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>e</mi><mi>w</mi></msub></mrow><annotation encoding="application/x-tex">e_w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span> are independently learned word embeddings, this is not a problem. But we observed that, when using a CNN, all the logits become tied as the function mapping from <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span></span></span></span></span> to <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>e</mi><mi>w</mi></msub></mrow><annotation encoding="application/x-tex">e_w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span> is quite smooth. As a result, a much smaller learning rate had to be used. Even with this, the model lacks capacity to differentiate between words that have very different meanings but that are spelled similarly. Thus, a reasonable compromise was to add a small correction factor which is learned per word, such that:</p>

<div class="math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>z</mi><mi>w</mi></msub><mo>=</mo><msup><mi>h</mi><mi>T</mi></msup><mi>C</mi><mi>N</mi><mi>N</mi><mo stretchy="false">(</mo><mi>c</mi><mi>h</mi><mi>a</mi><mi>r</mi><msub><mi>s</mi><mi>w</mi></msub><mo stretchy="false">)</mo><mo>+</mo><msup><mi>h</mi><mi>T</mi></msup><mi>M</mi><mi>c</mi><mi>o</mi><mi>r</mi><msub><mi>r</mi><mi>w</mi></msub></mrow><annotation encoding="application/x-tex">z_w=h^TCNN(chars_w) + h^T M corr_w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.044em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.1413em;vertical-align:-0.25em;"></span><span class="mord"><span class="var" data-var="h" data-desc="Number of parallel attention heads (h = 8)" style="--var-color: #ff7b72"><span class="mord mathnormal">h</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mopen">(</span><span class="mord mathnormal">c</span><span class="mord mathnormal">ha</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.0413em;vertical-align:-0.15em;"></span><span class="mord"><span class="var" data-var="h" data-desc="Number of parallel attention heads (h = 8)" style="--var-color: #ff7b72"><span class="mord mathnormal">h</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mord mathnormal" style="margin-right:0.02778em;">cor</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span></div>

<p>where <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span></span></span></span></span> is a matrix projecting a low-dimensional embedding vector <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi><mi>o</mi><mi>r</mi><msub><mi>r</mi><mi>w</mi></msub></mrow><annotation encoding="application/x-tex">corr_w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">cor</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span> back up to the dimensionality of the projected LSTM hidden state of <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi></mrow><annotation encoding="application/x-tex">h</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="var" data-var="h" data-desc="Number of parallel attention heads (h = 8)" style="--var-color: #ff7b72"><span class="mord mathnormal">h</span></span></span></span></span></span>. This amounts to adding a bottleneck linear layer, and brings the CNN Softmax much closer to our best result, as can be seen in Table~<a href="#main-results-table" class="cross-ref">Table 1</a>, where adding a 128-dim correction halves the gap between regular and the CNN Softmax.</p>

<p>Aside from a big reduction in the number of parameters and incorporating morphological knowledge from words, the other benefit of this approach is that out-of-vocabulary (OOV) words can easily be scored. This may be useful for other problems such as Machine Translation where handling out-of-vocabulary words is very important <sup class="ref-badge" data-ref="26" data-title="Wojciech." data-arxiv-id="1410.8206">26</sup>. This approach also allows parallel training over various data sets since the model is no longer explicitly parametrized by the vocabulary size -- or the language. This has shown to help when using byte-level input embeddings for named entity recognition <sup class="ref-badge" data-ref="12" data-title="Multilingual language processing from bytes." data-arxiv-id="1512.00103">12</sup>, and we hope it will enable similar gains when used to map onto words.</p>

<table class="article-table">
  <caption>Best results of single models on the 1B word benchmark. Our results are shown below previous work.</caption>
  <thead><tr>
    <th>\abovespace\belowspace
Model</th>
    <th>Test Perplexity</th>
    <th>Number of Params [billions]</th>
  </tr></thead>
  <tbody>
  <tr>
    <td>\abovespace
Sigmoid-RNN-2048 <sup class="ref-badge" data-ref="17" data-title="Dubey, Pradeep.">17</sup></td>
    <td>68.3</td>
    <td>4.1</td>
  </tr>
  <tr>
    <td>Interpolated KN 5-gram, 1.1B n-grams <sup class="ref-badge" data-ref="7" data-title="Koehn, Phillipp, and Robinson, Tony." data-arxiv-id="1312.3005">7</sup></td>
    <td>67.6</td>
    <td>1.76</td>
  </tr>
  <tr>
    <td>Sparse Non-Negative Matrix LM <sup class="ref-badge" data-ref="41" data-title="Sparse non-negative matrix language modeling for skip-grams.">41</sup></td>
    <td>52.9</td>
    <td>33</td>
  </tr>
  <tr>
    <td>RNN-1024 + MaxEnt 9-gram features <sup class="ref-badge" data-ref="7" data-title="Koehn, Phillipp, and Robinson, Tony." data-arxiv-id="1312.3005">7</sup></td>
    <td>51.3</td>
    <td>20</td>
  </tr>
  <tr>
    <td>\abovespace
LSTM-512-512</td>
    <td>54.1</td>
    <td>0.82</td>
  </tr>
  <tr>
    <td>LSTM-1024-512</td>
    <td>48.2</td>
    <td>0.82</td>
  </tr>
  <tr>
    <td>LSTM-2048-512</td>
    <td>43.7</td>
    <td>0.83</td>
  </tr>
  <tr>
    <td>LSTM-8192-2048 (No Dropout)</td>
    <td>37.9</td>
    <td>3.3</td>
  </tr>
  <tr>
    <td>LSTM-8192-2048 (50\% Dropout)</td>
    <td>32.2</td>
    <td>3.3</td>
  </tr>
  <tr>
    <td>2-Layer LSTM-8192-1024 (BIG LSTM)</td>
    <td>30.6</td>
    <td>1.8</td>
  </tr>
  <tr>
    <td>BIG LSTM+CNN Inputs</td>
    <td>**30.0**</td>
    <td>**1.04**</td>
  </tr>
  <tr>
    <td>\abovespace
BIG LSTM+CNN Inputs + CNN Softmax</td>
    <td>39.8</td>
    <td>**0.29**</td>
  </tr>
  <tr>
    <td>BIG LSTM+CNN Inputs + CNN Softmax + 128-dim correction</td>
    <td>35.8</td>
    <td>**0.39**</td>
  </tr>
  <tr>
    <td>BIG LSTM+CNN Inputs + Char LSTM predictions</td>
    <td>47.9</td>
    <td>**0.23**</td>
  </tr>
  </tbody>
</table>

### Char LSTM Predictions
<span id="charlstm" class="label-anchor"></span>

<p>The CNN Softmax layer can handle arbitrary words and is much more efficient in terms of number of parameters than the full Softmax matrix. It is, though, still considerably slow, as to evaluate perplexities we need to compute the partition function. A class of models that solve this problem more efficiently are character-level LSTMs <sup class="ref-badge" data-ref="45" data-title="Generating text with recurrent neural networks.">45</sup><sup class="ref-badge" data-ref="13" data-title="Generating sequences with recurrent neural networks." data-arxiv-id="1308.0850">13</sup>. They make predictions one character at a time, thus allowing to compute probabilities over a much smaller vocabulary. On the other hand, these models are more difficult to train and seem to perform worse even in small tasks like PTB <sup class="ref-badge" data-ref="13" data-title="Generating sequences with recurrent neural networks." data-arxiv-id="1308.0850">13</sup>. Most likely this is due to the sequences becoming much longer on average as the LSTM reads the input character by character instead of word by word.</p>

<p>Thus, we combine the word and character-level models by feeding a word-level LSTM hidden state <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi></mrow><annotation encoding="application/x-tex">h</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="var" data-var="h" data-desc="Number of parallel attention heads (h = 8)" style="--var-color: #ff7b72"><span class="mord mathnormal">h</span></span></span></span></span></span> into a small LSTM that predicts the target word one character at a time (see Figure~<a href="#main_figure" class="cross-ref">Fig. 1</a>(c)). In order to make the whole process reasonably efficient, we train the standard LSTM model until convergence, freeze its weights, and replace the standard word-level Softmax layer with the aforementioned character-level LSTM.</p>

<p>The resulting model scales independently of vocabulary size -- both for training and inference. However, it does seem to be worse than regular and CNN Softmax -- we are hopeful that further research will enable these models to replace fixed vocabulary models whilst being computationally attractive.</p>

<hr>

<h2>4. Experiments</h2>

<span id="exps" class="label-anchor"></span>

<p>All experiments were run using the TensorFlow system <sup class="ref-badge" data-ref="1" data-title="Zhifeng, Citro, Craig, Corrado, Greg S., Davis, Andy, Dean, Jeffrey, Devin,">1</sup>, with the exception of some older models which were used in the ensemble.</p>

### Data Set
<span id="data set" class="label-anchor"></span>

<p>The experiments are performed on the 1B Word Benchmark data set introduced by <sup class="ref-badge" data-ref="7" data-title="Koehn, Phillipp, and Robinson, Tony." data-arxiv-id="1312.3005">7</sup>, which is a publicly available benchmark for measuring progress of statistical language modeling. The data set contains about 0.8B words with a vocabulary of 793471 words, including sentence boundary markers. All the sentences are shuffled and the duplicates are removed. The words that are out of vocabulary (OOV) are marked with a special UNK token (there are approximately 0.3\% such words).</p>

### Model Setup
<span id="experiments" class="label-anchor"></span>

<p>The typical measure used for reporting progress in language modeling is perplexity, which is the average per-word log-probability on the holdout data set: <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>e</mi><mrow><mo>−</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><msub><mo>∑</mo><mi>i</mi></msub><mi>ln</mi><mo>⁡</mo><msub><mi>p</mi><msub><mi>w</mi><mi>i</mi></msub></msub></mrow></msup></mrow><annotation encoding="application/x-tex">e^{-\frac{1}{N}\sum_i \ln{p_{w_i}}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9562em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9562em;"><span style="top:-3.3652em;margin-right:0.05em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight"><span class="mopen nulldelimiter sizing reset-size3 size6"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8443em;"><span style="top:-2.656em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span><span style="top:-3.2255em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line mtight" style="border-bottom-width:0.049em;"></span></span><span style="top:-3.384em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.344em;"><span></span></span></span></span></span><span class="mclose nulldelimiter sizing reset-size3 size6"></span></span><span class="mspace mtight" style="margin-right:0.1952em;"></span><span class="mop mtight"><span class="mop op-symbol small-op mtight" style="position:relative;top:0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1496em;"><span style="top:-2.1786em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3214em;"><span></span></span></span></span></span></span><span class="mspace mtight" style="margin-right:0.1952em;"></span><span class="mop mtight"><span class="mtight">l</span><span class="mtight">n</span></span><span class="mspace mtight" style="margin-right:0.1952em;"></span><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1645em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3448em;margin-left:-0.0269em;margin-right:0.1em;"><span class="pstrut" style="height:2.6595em;"></span><span class="mord mathnormal mtight">i</span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3147em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3678em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span>. We follow the standard procedure and sum over all the words (including the end of sentence symbol).</p>

<p>We used the 1B Word Benchmark data set without any pre-processing. Given the shuffled sentences, they are input to the network as a batch of independent streams of words. Whenever a sentence ends, a new one starts without any padding (thus maximizing the occupancy per batch).</p>

<p>For the models that consume characters as inputs or as targets, each word is fed to the model as a sequence of character IDs of preespecified length (see Figure~<a href="#main_figure" class="cross-ref">Fig. 1</a>(b)). The words were processed to include special begin and end of word tokens and were padded to reach the expected length. I.e. if the maximum word length was 10, the word ``\texttt{cat}'' would be transformed to ``\texttt{\$cat\^{}               }'' due to the CNN model.</p>

<p>In our experiments we found that limiting the maximum word length in training to 50 was sufficient to reach very good results while 32 was clearly insufficient. We used 256 characters in our vocabulary and the non-ascii symbols were represented as a sequence of bytes.</p>

### Model Architecture

<p>We evaluated many variations of RNN LM architectures. These include the dimensionalities of the embedding layers, the state, projection sizes, and number of LSTM layers to use. Exhaustively trying all combinations would be extremely time consuming for such a large data set, but our findings suggest that LSTMs with a projection layer (i.e., a bottleneck between hidden states as in <sup class="ref-badge" data-ref="37" data-title="Long short-term memory recurrent neural network architectures for">37</sup>) trained with truncated BPTT <sup class="ref-badge" data-ref="51" data-title="An efficient gradient-based algorithm for on-line training of">51</sup> for 20 steps performed well.</p>

<p>Following <sup class="ref-badge" data-ref="53" data-title="Recurrent neural network regularization." data-arxiv-id="1409.2329">53</sup> we use dropout <sup class="ref-badge" data-ref="42" data-title="Improving neural networks with dropout.">42</sup> before and after every LSTM layer. The biases of LSTM forget gate were initialized to 1.0 <sup class="ref-badge" data-ref="19" data-title="An empirical exploration of recurrent network architectures.">19</sup>. The size of the models will be described in more detail in the following sections, and the choices of hyper-parameters will be released as open source upon publication.</p>

<p>For any model using character embedding CNNs, we closely follow the architecture from <sup class="ref-badge" data-ref="21" data-title="Character-aware neural language models." data-arxiv-id="1508.06615">21</sup>. The only important difference is that we use a larger number of convolutional features of 4096 to give enough capacity to the model. The resulting embedding is then linearly transformed to match the LSTM projection sizes. This allows it to match the performance of regular word embeddings but only uses a small fraction of parameters.</p>

<table class="article-table">
  <caption>Best results of ensembles on the 1B Word Benchmark.</caption>
  <thead><tr>
    <th>\abovespace\belowspace
Model</th>
    <th>Test Perplexity</th>
  </tr></thead>
  <tbody>
  <tr>
    <td>\abovespace
Large Ensemble <sup class="ref-badge" data-ref="7" data-title="Koehn, Phillipp, and Robinson, Tony." data-arxiv-id="1312.3005">7</sup></td>
    <td>43.8</td>
  </tr>
  <tr>
    <td>RNN+KN-5 <sup class="ref-badge" data-ref="52" data-title="Scaling recurrent neural network language models.">52</sup></td>
    <td>42.4</td>
  </tr>
  <tr>
    <td>RNN+KN-5 <sup class="ref-badge" data-ref="17" data-title="Dubey, Pradeep.">17</sup></td>
    <td>42.0</td>
  </tr>
  <tr>
    <td>RNN+SNM10-skip <sup class="ref-badge" data-ref="41" data-title="Sparse non-negative matrix language modeling for skip-grams.">41</sup></td>
    <td>41.3</td>
  </tr>
  <tr>
    <td>Large Ensemble <sup class="ref-badge" data-ref="41" data-title="Sparse non-negative matrix language modeling for skip-grams.">41</sup></td>
    <td>41.0</td>
  </tr>
  <tr>
    <td>\abovespace
Our 10 best LSTM models (equal weights)</td>
    <td>26.3</td>
  </tr>
  <tr>
    <td>Our 10 best LSTM models (optimal weights)</td>
    <td>26.1</td>
  </tr>
  <tr>
    <td>10 LSTMs + KN-5 (equal weights)</td>
    <td>25.3</td>
  </tr>
  <tr>
    <td>10 LSTMs + KN-5 (optimal weights)</td>
    <td>25.1</td>
  </tr>
  <tr>
    <td>10 LSTMs + SNM10-skip <sup class="ref-badge" data-ref="41" data-title="Sparse non-negative matrix language modeling for skip-grams.">41</sup></td>
    <td>**23.7**</td>
  </tr>
  </tbody>
</table>

### Training Procedure

<p>The models were trained until convergence with an AdaGrad optimizer using a learning rate of 0.2. In all the experiments the RNNs were unrolled for 20 steps without ever resetting the LSTM states. We used a batch size of 128. We clip the gradients of the LSTM weights such that their norm is bounded by 1.0 <sup class="ref-badge" data-ref="35" data-title="On the difficulty of training recurrent neural networks." data-arxiv-id="1211.5063">35</sup>.</p>

<p>Using these hyper-parameters we found large LSTMs to be relatively easy to train. The same learning rate was used in almost all of the experiments. In a few cases we had to reduce it by an order of magnitude. Unless otherwise stated, the experiments were performed with 32 GPU workers and asynchronous gradient updates. Further details will be fully specified with the code upon publication.</p>

<p>Training a model for such large target vocabulary (793471 words) required to be careful with some details about the approximation to full Softmax using importance sampling. We used a large number of negative (or noise) samples: 8192 such samples were drawn per step, but were shared across all the target words in the batch (2560 total, i.e. 128 times 20 unrolled steps). This results in multiplying (2560 x 1024) times (1024 x (8192+1)) (instead of (2560 x 1024) times (1024 x 793471)), i.e. about 100-fold less computation.</p>

<hr>

<h2>5. Results and Analysis</h2>

<span id="results" class="label-anchor"></span>

<p>In this section we summarize the results of our experiments and do an in-depth analysis. Table~<a href="#main-results-table" class="cross-ref">Table 1</a> contains all results for our models compared to previously published work. Table~<a href="#ensemble-results-table" class="cross-ref">Table 2</a> shows previous and our own work on ensembles of models. We hope that our encouraging results, which improved the best perplexity of a single model from 51.3 to 30.0 (whilst reducing the model size considerably), and set a new record with ensembles at 23.7, will enable rapid research and progress to advance Language Modeling. For this purpose, we will release the model weights and recipes upon publication.</p>

### Size Matters

<p>Unsurprisingly, size matters: when training on a very large and complex data set, fitting the training data with an LSTM is fairly challenging. Thus, the size of the LSTM layer is a very important factor that influences the results, as seen in Table~<a href="#main-results-table" class="cross-ref">Table 1</a>. The best models are the largest we were able to fit into a GPU memory. Our largest model was a 2-layer LSTM with 8192+1024 dimensional recurrent state in each of the layers. Increasing the embedding and projection size also helps but causes a large increase in the number of parameters, which is less desirable. Lastly, training an RNN instead of an LSTM yields poorer results (about 5 perplexity worse) for a comparable model size.</p>

### Regularization Importance

<p>As shown in Table~<a href="#main-results-table" class="cross-ref">Table 1</a>, using dropout improves the results. To our surprise, even relatively small models (e.g., single layer LSTM with 2048 units projected to 512 dimensional outputs) can over-fit the training set if trained long enough, eventually yielding holdout set degradation.</p>

<p>Using dropout on non-recurrent connections largely mitigates these issues. While over-fitting still occurs, there is no more need for early stopping. For models that had 4096 or less units in the LSTM layer, we used 10\% dropout probability. For larger models, 25\% was significantly better. Even with such regularization, perplexities on the training set can be as much as 6 points below test.</p>

<p>In one experiment we tried to use a smaller vocabulary comprising of the 100,000 most frequent words and found the difference between train and test to be smaller -- which suggests that too much capacity is given to rare words. This is less of an issue with character CNN embedding models as the embeddings are shared across all words.</p>

### Importance Sampling is Data Efficient
Table~<a href="#nce-vs-sampled" class="cross-ref">Table 3</a> shows the test perplexities of NCE vs IS loss after a few epochs of 2048 unit LSTM with 512 projection. The IS objective significantly improves the speed and the overall performance of the model when compared to NCE.

<table class="article-table">
  <caption>The test perplexities of an LSTM-2048-512 trained with different losses versus number of epochs. The model needs about 40 minutes per epoch. First epoch is a bit slower because we slowly increase the number of workers.</caption>
  <thead><tr>
    <th>\abovespace\belowspace
Epochs</th>
    <th>NCE</th>
    <th>IS</th>
    <th>Training Time [Hours]</th>
  </tr></thead>
  <tbody>
  <tr>
    <td>\abovespace
1</td>
    <td>97</td>
    <td>60</td>
    <td>1</td>
  </tr>
  <tr>
    <td>5</td>
    <td>58</td>
    <td>47.5</td>
    <td>4</td>
  </tr>
  <tr>
    <td>10</td>
    <td>53</td>
    <td>45</td>
    <td>8</td>
  </tr>
  <tr>
    <td>20</td>
    <td>49</td>
    <td>44</td>
    <td>14</td>
  </tr>
  <tr>
    <td>50</td>
    <td>46.1</td>
    <td>43.7</td>
    <td>34</td>
  </tr>
  </tbody>
</table>

### Word Embeddings vs Character CNN
Replacing the embedding layer with a parametrized neural network that process characters of a given word allows the model to consume arbitrary words and is not restricted to a fixed vocabulary. This property is useful for data sets with conversational or informal text as well as for morphologically rich languages. Our experiments show that using character-level embeddings is feasible and does not degrade performance -- in fact, our best single model uses a Character CNN embedding.

<p>An additional advantage is that the number of parameters of the input layer is reduced by a factor of 11 (though training speed is slightly worse). For inference, the embeddings can be precomputed so there is no speed penalty. Overall, the embedding of the best model is parametrized by 72M weights (down from 820M weights).</p>

<p>Table~<a href="#nearest-neighbors" class="cross-ref">Table 4</a> shows a few examples of nearest neighbor embeddings for some out-of-vocabulary words when character CNNs are used.</p>

### Smaller Models with CNN Softmax

<table class="article-table">
  <caption>Nearest neighbors in the character CNN embedding space of a few out-of-vocabulary words. Even for words that the model has never seen, the model usually still finds reasonable neighbors.</caption>
  <thead><tr>
    <th>\abovespace\belowspace
Word</th>
    <th>Top-1</th>
    <th>Top-2</th>
    <th>Top-3</th>
  </tr></thead>
  <tbody>
  <tr>
    <td>\abovespace
incerdible</td>
    <td>incredible</td>
    <td>nonedible</td>
    <td>extendible</td>
  </tr>
  <tr>
    <td>www.a.com</td>
    <td>www.aa.com</td>
    <td>www.aaa.com</td>
    <td>www.ca.com</td>
  </tr>
  <tr>
    <td>7546</td>
    <td>7646</td>
    <td>7534</td>
    <td>8566</td>
  </tr>
  <tr>
    <td>TownHal1</td>
    <td>TownHall</td>
    <td>DJc2</td>
    <td>Moodswing360</td>
  </tr>
  <tr>
    <td>Komarski</td>
    <td>Koharski</td>
    <td>Konarski</td>
    <td>Komanski</td>
  </tr>
  </tbody>
</table>

<p>Even with character-level embeddings, the model is still fairly large (though much smaller than the best competing models from previous work). Most of the parameters are in the linear layer before the Softmax: 820M versus a total of 1.04B parameters.</p>

<p>In one of the experiments we froze the word-LSTM after convergence and replaced the Softmax layer with the CNN Softmax sub-network. Without any fine-tuning that model was able to reach 39.8 perplexity with only 293M weights (as seen in Table~<a href="#main-results-table" class="cross-ref">Table 1</a>).</p>

<p>As described in Section~<a href="#cnn_softmax" class="cross-ref">Section 8</a>, adding a ``correction'' word embedding term alleviates the gap between regular and CNN Softmax. Indeed, we can trade-off model size versus perplexity. For instance, by adding 100M weights (through a 128 dimensional bottleneck embedding) we achieve 35.8 perplexity (see Table~<a href="#main-results-table" class="cross-ref">Table 1</a>).</p>

<p>To contrast with the CNN Softmax, we also evaluated a model that replaces the Softmax layer with a smaller LSTM that predicts one character at a time (see Section~<a href="#charlstm" class="cross-ref">Section 9</a>). Such a model does not have to learn long dependencies because the base LSTM still operates at the word-level (see Figure~<a href="#main_figure" class="cross-ref">Fig. 1</a>(c)). With a single-layer LSTM of 1024 units we reached 49.0 test perplexity, far below the best model. In order to make the comparisons more fair, we performed a very expensive marginalization over the words in the vocabulary (to rule out words not in the dictionary which the character LSTM would assign some probability). When doing this marginalization, the perplexity improved a bit down to 47.9.</p>

<figure id="main_figure">
<img src="./figures/Figure1_LMPAPER.png" alt="A high-level diagram of the models presented in this paper. (a) is a standard LSTM LM. (b) represents an LM where both input and Softmax embeddings have been replaced by a character CNN. In (c) we replace the Softmax by a next character prediction LSTM network." loading="lazy" style="max-width:100%">
<figcaption><strong>Figure 1.</strong> A high-level diagram of the models presented in this paper. (a) is a standard LSTM LM. (b) represents an LM where both input and Softmax embeddings have been replaced by a character CNN. In (c) we replace the Softmax by a next character prediction LSTM network.</figcaption>
</figure>

### Training Speed

<p>We used 32 Tesla K40 GPUs to train our models. The smaller version of the LSTM model with 2048 units and 512 projections needs less than 10 hours to reach below 45 perplexity and after only **2 hours** of training the model beats previous state-of-the art on this data set. The best model needs about 5 days to get to 35 perplexity and 10 days to 32.5. The best results were achieved after 3 weeks of training. See Table~<a href="#nce-vs-sampled" class="cross-ref">Table 3</a> for more details.</p>

### Ensembles

<p>We averaged several of our best models and we were able to reach 23.7 test perplexity (more details and results can be seen in Table~<a href="#ensemble-results-table" class="cross-ref">Table 2</a>), which is more than 40\% improvement over previous work. Interestingly, including the best N-gram model reduces the perplexity by 1.2 point even though the model is rather weak on its own (67.6 perplexity). Most previous work had to either ensemble with the best N-gram model (as their RNN only used a limited output vocabulary of a few thousand words), or use N-gram features as additional input to the RNN. Our results, on the contrary, suggest that N-grams are of limited benefit, and suggest that a carefully trained LSTM LM is the most competitive model.</p>

### LSTMs are best on the tail words

<p>Figure~<a href="#tail-words" class="cross-ref">Fig. 2</a> shows the difference in log probabilities between our best model (at 30.0 perplexity) and the KN-5. As can be seen from the plot, the LSTM is better across all the buckets and significantly outperforms KN-5 on the rare words. This is encouraging as it seems to suggest that LSTM LMs may fare even better for languages or data sets where the number of rare words is larger than traditional N-gram models.</p>

### Samples from the model

<p>To qualitatively evaluate the model, we sampled many sentences. We discarded short and politically incorrect ones, but the sample shown below is otherwise ``raw'' (i.e., not hand picked). The samples are of high quality -- which is not a surprise, given the perplexities attained -- but there are still some occasional mistakes.</p>

<p>Sentences generated by the ensemble (about 26 perplexity):
\begin{quotation} \noindent \scriptsize
<span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow></mrow><annotation encoding="application/x-tex"></annotation></semantics></math></span><span class="katex-html" aria-hidden="true"></span></span></span> With even more new technologies coming onto the market quickly during the past three years , an increasing number of companies now must tackle the ever-changing and ever-changing environmental challenges online .
<span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow></mrow><annotation encoding="application/x-tex"></annotation></semantics></math></span><span class="katex-html" aria-hidden="true"></span></span></span> Check back for updates on this breaking news story .
<span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow></mrow><annotation encoding="application/x-tex"></annotation></semantics></math></span><span class="katex-html" aria-hidden="true"></span></span></span> About 800 people gathered at Hever Castle on Long Beach from noon to 2pm , three to four times that of the funeral cort\`ege .
<span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow></mrow><annotation encoding="application/x-tex"></annotation></semantics></math></span><span class="katex-html" aria-hidden="true"></span></span></span> We are aware of written instructions from the copyright holder not to , in any way , mention Rosenberg 's negative comments if they are relevant as indicated in the documents , " eBay said in a statement .
<span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow></mrow><annotation encoding="application/x-tex"></annotation></semantics></math></span><span class="katex-html" aria-hidden="true"></span></span></span> It is now known that coffee and cacao products can do no harm on the body .
<span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow></mrow><annotation encoding="application/x-tex"></annotation></semantics></math></span><span class="katex-html" aria-hidden="true"></span></span></span> Yuri Zhirkov was in attendance at the Stamford Bridge at the start of the second half but neither Drogba nor Malouda was able to push on through the Barcelona defence .
\end{quotation}</p>

<hr>

<h2>6. Discussion and Conclusions</h2>

<span id="discussion" class="label-anchor"></span>

<p>In this paper we have shown that RNN LMs can be trained on large amounts of data, and outperform competing models including carefully tuned N-grams. The reduction in perplexity from 51.3 to 30.0 is due to several key components which we studied in this paper. Thus, a large, regularized LSTM LM, with projection layers and trained with an approximation to the true Softmax with importance sampling performs much better than N-grams. Unlike previous work, we do not require to interpolate both the RNN LM and the N-gram, and the gains of doing so are rather marginal.</p>

<p>By exploring recent advances in model architectures (e.g. LSTMs), exploiting small character CNNs, and by sharing our findings in this paper and accompanying code and models (to be released upon publication), we hope to inspire research on large scale Language Modeling, a problem we consider crucial towards language understanding. We hope for future research to focus on reasonably sized datasets taking inspiration from recent advances seen in the computer vision community thanks to efforts such as Imagenet <sup class="ref-badge" data-ref="9" data-title="Imagenet: A large-scale hierarchical image database.">9</sup>.</p>

<hr>

<h2>7. Acknowledgements</h2>

<p>We thank Ciprian Chelba, Ilya Sutskever, and the Google Brain Team for their help and discussions. We also thank Koray Kavukcuoglu for his help with the manuscript.</p>

<p>\bibliography{lm_paper}
\bibliographystyle{icml2016}</p>

<hr>

<h2>References</h2>
<ol class="references">
  <li id="ref-1"><strong>Abadi, Mart\'\in, Agarwal, Ashish, Barham, Paul, Brevdo, Eugene, Chen,</strong> Zhifeng, Citro, Craig, Corrado, Greg S., Davis, Andy, Dean, Jeffrey, Devin,. <em>Matthieu, Ghemawat, Sanjay, Goodfellow, Ian, Harp, Andrew, Irving, Geoffrey, Isard, Michael, Jia, Yangqing, Jozefowicz, Rafal, Kaiser, Lukasz, Kudlur, Manjunath, Levenberg, Josh, Man\'e, Dan, Monga, Rajat, Moore, Sherry, Murray, Derek, Olah, Chris, Schuster, Mike, Shlens, Jonathon, Steiner, Benoit, Sutskever, Ilya, Talwar, Kunal, Tucker, Paul, Vanhoucke, Vincent, Vasudevan, Vijay, Vi\'egas, Fernanda, Vinyals, Oriol, Warden, Pete, Wattenberg, Martin, Wicke, Martin, Yu, Yuan, and Zheng, Xiaoqiang. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. URL http://tensorflow.org/. Software available from tensorflow.org.</em></li>
  <li id="ref-2"><strong>Arisoy, Ebru, Sainath, Tara N, Kingsbury, Brian, and Ramabhadran, Bhuvana.</strong> Deep neural network language models.. <em>In Proceedings of the NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for HLT, pp.\  20--28. Association for Computational Linguistics, 2012.</em></li>
  <li id="ref-3"><strong>Ballesteros, Miguel, Dyer, Chris, and Smith, Noah A.</strong> Improved transition-based parsing by modeling characters instead of. <em>words with lstms. arXiv preprint arXiv:1508.00657, 2015.</em></li>
  <li id="ref-4"><strong>Bengio, Yoshua and Sen\'ecal, Jean-S\'ebastien.</strong> Adaptive importance sampling to accelerate training of a neural. <em>probabilistic language model. Neural Networks, IEEE Transactions on, 19\penalty0 (4):\penalty0 713--722, 2008.</em></li>
  <li id="ref-5"><strong>Bengio, Yoshua, Sen\'ecal, Jean-S\'ebastien, et al.</strong> Quick training of probabilistic neural nets by importance sampling.. <em>In AISTATS, 2003.</em></li>
  <li id="ref-6"><strong>Bengio, Yoshua, Schwenk, Holger, Sen\'ecal, Jean-S\'ebastien, Morin,</strong> Fr\'ederic, and Gauvain, Jean-Luc.. <em>Neural probabilistic language models. In Innovations in Machine Learning, pp.\  137--186. Springer, 2006.</em></li>
  <li id="ref-7"><strong>Chelba, Ciprian, Mikolov, Tomas, Schuster, Mike, Ge, Qi, Brants, Thorsten,</strong> Koehn, Phillipp, and Robinson, Tony.. <em>One billion word benchmark for measuring progress in statistical language modeling. arXiv preprint arXiv:1312.3005, 2013.</em></li>
  <li id="ref-8"><strong>Cho, Kyunghyun, Van Merri\"enboer, Bart, Gulcehre, Caglar, Bahdanau, Dzmitry,</strong> Bougares, Fethi, Schwenk, Holger, and Bengio, Yoshua.. <em>Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014.</em></li>
  <li id="ref-9"><strong>Deng, Jia, Dong, Wei, Socher, Richard, Li, Li-Jia, Li, Kai, and Fei-Fei, Li.</strong> Imagenet: A large-scale hierarchical image database.. <em>In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pp.\  248--255. IEEE, 2009.</em></li>
  <li id="ref-10"><strong>Filippova, Katja, Alfonseca, Enrique, Colmenares, Carlos A, Kaiser, Lukasz, and</strong> Vinyals, Oriol.. <em>Sentence compression by deletion with lstms. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pp.\  360--368, 2015.</em></li>
  <li id="ref-11"><strong>Gers, Felix A, Schmidhuber, J\"urgen, and Cummins, Fred.</strong> Learning to forget: Continual prediction with lstm.. <em>Neural computation, 12\penalty0 (10):\penalty0 2451--2471, 2000.</em></li>
  <li id="ref-12"><strong>Gillick, Dan, Brunk, Cliff, Vinyals, Oriol, and Subramanya, Amarnag.</strong> Multilingual language processing from bytes.. <em>arXiv preprint arXiv:1512.00103, 2015.</em></li>
  <li id="ref-13"><strong>Graves, Alex.</strong> Generating sequences with recurrent neural networks.. <em>arXiv preprint arXiv:1308.0850, 2013.</em></li>
  <li id="ref-14"><strong>Graves, Alex and Schmidhuber, J\"urgen.</strong> Framewise phoneme classification with bidirectional lstm and other. <em>neural network architectures. Neural Networks, 18\penalty0 (5):\penalty0 602--610, 2005.</em></li>
  <li id="ref-15"><strong>Gutmann, Michael and Hyv\"arinen, Aapo.</strong> Noise-contrastive estimation: A new estimation principle for. <em>unnormalized statistical models. In International Conference on Artificial Intelligence and Statistics, pp.\  297--304, 2010.</em></li>
  <li id="ref-16"><strong>Hochreiter, Sepp and Schmidhuber, J\"urgen.</strong> Long short-term memory.. <em>Neural computation, 9\penalty0 (8):\penalty0 1735--1780, 1997.</em></li>
  <li id="ref-17"><strong>Ji, Shihao, Vishwanathan, S. V. N., Satish, Nadathur, Anderson, Michael J., and</strong> Dubey, Pradeep.. <em>Blackout: Speeding up recurrent neural network language models with very large vocabularies. CoRR, abs/1511.06909, 2015\natexlaba. URL http://arxiv.org/abs/1511.06909.</em></li>
  <li id="ref-18"><strong>Ji, Yangfeng, Cohn, Trevor, Kong, Lingpeng, Dyer, Chris, and Eisenstein, Jacob.</strong> Document context language models.. <em>arXiv preprint arXiv:1511.03962, 2015\natexlabb.</em></li>
  <li id="ref-19"><strong>Jozefowicz, Rafal, Zaremba, Wojciech, and Sutskever, Ilya.</strong> An empirical exploration of recurrent network architectures.. <em>In Proceedings of the 32nd International Conference on Machine Learning (ICML-15), pp.\  2342--2350, 2015.</em></li>
  <li id="ref-20"><strong>Kalchbrenner, Nal, Grefenstette, Edward, and Blunsom, Phil.</strong> A convolutional neural network for modelling sentences.. <em>arXiv preprint arXiv:1404.2188, 2014.</em></li>
  <li id="ref-21"><strong>Kim, Yoon, Jernite, Yacine, Sontag, David, and Rush, Alexander M.</strong> Character-aware neural language models.. <em>arXiv preprint arXiv:1508.06615, 2015.</em></li>
  <li id="ref-22"><strong>Kneser, Reinhard and Ney, Hermann.</strong> Improved backing-off for m-gram language modeling.. <em>In Acoustics, Speech, and Signal Processing, 1995. ICASSP-95., 1995 International Conference on, volume 1, pp.\  181--184. IEEE, 1995.</em></li>
  <li id="ref-23"><strong>Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E.</strong> Imagenet classification with deep convolutional neural networks.. <em>In Advances in neural information processing systems, pp.\ 1097--1105, 2012.</em></li>
  <li id="ref-24"><strong>Le Cun, B Boser, Denker, John S, Henderson, D, Howard, Richard E, Hubbard, W,</strong> and Jackel, Lawrence D.. <em>Handwritten digit recognition with a back-propagation network. In Advances in neural information processing systems. Citeseer, 1990.</em></li>
  <li id="ref-25"><strong>Ling, Wang, Lu\'\is, Tiago, Marujo, Lu\'\is, Astudillo,</strong> Ram\'on Fernandez, Amir, Silvio, Dyer, Chris, Black, Alan W, and Trancoso,. <em>Isabel. Finding function in form: Compositional character models for open vocabulary word representation. arXiv preprint arXiv:1508.02096, 2015.</em></li>
  <li id="ref-26"><strong>Luong, Minh-Thang, Sutskever, Ilya, Le, Quoc V, Vinyals, Oriol, and Zaremba,</strong> Wojciech.. <em>Addressing the rare word problem in neural machine translation. arXiv preprint arXiv:1410.8206, 2014.</em></li>
  <li id="ref-27"><strong>Marcus, Mitchell P, Marcinkiewicz, Mary Ann, and Santorini, Beatrice.</strong> Building a large annotated corpus of english: The penn treebank.. <em>Computational linguistics, 19\penalty0 (2):\penalty0 313--330, 1993.</em></li>
  <li id="ref-28"><strong>Mikolov, Tom\'a\vs.</strong> Statistical language models based on neural networks.. <em>Presentation at Google, Mountain View, 2nd April, 2012.</em></li>
  <li id="ref-29"><strong>Mikolov, Tomas and Zweig, Geoffrey.</strong> Context dependent recurrent neural network language model.. <em>In SLT, pp.\  234--239, 2012.</em></li>
  <li id="ref-30"><strong>Mikolov, Tomas, Karafi\'at, Martin, Burget, Lukas, Cernock\`y, Jan, and</strong> Khudanpur, Sanjeev.. <em>Recurrent neural network based language model. In INTERSPEECH, volume 2, pp.\  3, 2010.</em></li>
  <li id="ref-31"><strong>Mikolov, Tomas, Deoras, Anoop, Kombrink, Stefan, Burget, Lukas, and</strong> Cernock\`y, Jan.. <em>Empirical evaluation and combination of advanced language modeling techniques. In INTERSPEECH, number s 1, pp.\  605--608, 2011.</em></li>
  <li id="ref-32"><strong>Mnih, Andriy and Hinton, Geoffrey E.</strong> A scalable hierarchical distributed language model.. <em>In Advances in neural information processing systems, pp.\ 1081--1088, 2009.</em></li>
  <li id="ref-33"><strong>Mnih, Andriy and Kavukcuoglu, Koray.</strong> Learning word embeddings efficiently with noise-contrastive. <em>estimation. In Advances in Neural Information Processing Systems, pp.\ 2265--2273, 2013.</em></li>
  <li id="ref-34"><strong>Morin, Frederic and Bengio, Yoshua.</strong> Hierarchical probabilistic neural network language model.. <em>In Aistats, volume 5, pp.\  246--252. Citeseer, 2005.</em></li>
  <li id="ref-35"><strong>Pascanu, Razvan, Mikolov, Tomas, and Bengio, Yoshua.</strong> On the difficulty of training recurrent neural networks.. <em>arXiv preprint arXiv:1211.5063, 2012.</em></li>
  <li id="ref-36"><strong>Rush, Alexander M, Chopra, Sumit, and Weston, Jason.</strong> A neural attention model for abstractive sentence summarization.. <em>arXiv preprint arXiv:1509.00685, 2015.</em></li>
  <li id="ref-37"><strong>Sak, Hasim, Senior, Andrew W, and Beaufays, Fran\ccoise.</strong> Long short-term memory recurrent neural network architectures for. <em>large scale acoustic modeling. In INTERSPEECH, pp.\  338--342, 2014.</em></li>
  <li id="ref-38"><strong>Schuster, Mike and Paliwal, Kuldip K.</strong> Bidirectional recurrent neural networks.. <em>Signal Processing, IEEE Transactions on, 45\penalty0 (11):\penalty0 2673--2681, 1997.</em></li>
  <li id="ref-39"><strong>Schwenk, Holger, Rousseau, Anthony, and Attik, Mohammed.</strong> Large, pruned or continuous space language models on a gpu for. <em>statistical machine translation. In Proceedings of the NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for HLT, pp.\  11--19. Association for Computational Linguistics, 2012.</em></li>
  <li id="ref-40"><strong>Serban, Iulian Vlad, Sordoni, Alessandro, Bengio, Yoshua, Courville, Aaron C.,</strong> and Pineau, Joelle.. <em>Hierarchical neural network generative models for movie dialogues. CoRR, abs/1507.04808, 2015. URL http://arxiv.org/abs/1507.04808.</em></li>
  <li id="ref-41"><strong>Shazeer, Noam, Pelemans, Joris, and Chelba, Ciprian.</strong> Sparse non-negative matrix language modeling for skip-grams.. <em>Proceedings of Interspeech, pp.\  1428--1432, 2015.</em></li>
  <li id="ref-42"><strong>Srivastava, Nitish.</strong> Improving neural networks with dropout.. <em>PhD thesis, University of Toronto, 2013.</em></li>
  <li id="ref-43"><strong>Srivastava, Nitish, Mansimov, Elman, and Salakhutdinov, Ruslan.</strong> Unsupervised learning of video representations using lstms.. <em>arXiv preprint arXiv:1502.04681, 2015\natexlaba.</em></li>
  <li id="ref-44"><strong>Srivastava, Rupesh K, Greff, Klaus, and Schmidhuber, J\"urgen.</strong> Training very deep networks.. <em>In Advances in Neural Information Processing Systems, pp.\ 2368--2376, 2015\natexlabb.</em></li>
  <li id="ref-45"><strong>Sutskever, Ilya, Martens, James, and Hinton, Geoffrey E.</strong> Generating text with recurrent neural networks.. <em>In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pp.\  1017--1024, 2011.</em></li>
  <li id="ref-46"><strong>Sutskever, Ilya, Vinyals, Oriol, and Le, Quoc V.</strong> Sequence to sequence learning with neural networks.. <em>In Advances in neural information processing systems, pp.\ 3104--3112, 2014.</em></li>
  <li id="ref-47"><strong>Vaswani, Ashish, Zhao, Yinggong, Fossum, Victoria, and Chiang, David.</strong> Decoding with large-scale neural language models improves. <em>translation. Citeseer.</em></li>
  <li id="ref-48"><strong>Vincent, Pascal, de Br\'ebisson, Alexandre, and Bouthillier, Xavier.</strong> Efficient exact gradient update for training deep networks with very. <em>large sparse targets. In Advances in Neural Information Processing Systems, pp.\ 1108--1116, 2015.</em></li>
  <li id="ref-49"><strong>Vinyals, Oriol and Le, Quoc.</strong> A neural conversational model.. <em>arXiv preprint arXiv:1506.05869, 2015.</em></li>
  <li id="ref-50"><strong>Wang, Tian and Cho, Kyunghyun.</strong> Larger-context language modelling.. <em>arXiv preprint arXiv:1511.03729, 2015.</em></li>
  <li id="ref-51"><strong>Williams, Ronald J and Peng, Jing.</strong> An efficient gradient-based algorithm for on-line training of. <em>recurrent network trajectories. Neural computation, 2\penalty0 (4):\penalty0 490--501, 1990.</em></li>
  <li id="ref-52"><strong>Williams, Will, Prasad, Niranjani, Mrva, David, Ash, Tom, and Robinson, Tony.</strong> Scaling recurrent neural network language models.. <em>In Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on, pp.\  5391--5395. IEEE, 2015.</em></li>
  <li id="ref-53"><strong>Zaremba, Wojciech, Sutskever, Ilya, and Vinyals, Oriol.</strong> Recurrent neural network regularization.. <em>arXiv preprint arXiv:1409.2329, 2014. \endthebibliography</em></li>
</ol>
