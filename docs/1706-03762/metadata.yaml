title: Attention Is All You Need
short_title: Attention Is All You Need
type: journal-article
authors:
  - given: with
    family: Illia
  - given: >-
      designed and implemented the first Transformer models and has been crucially involved in every
      aspect of this work. Noam proposed scaled dot-product
    family: attention
  - given: >-
      multi-head attention and the parameter-free position representation and became the other
      person involved in nearly every detail. Niki
    family: designed
  - given: ''
    family: implemented
  - given: >-
      tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion
      also experimented with novel model
    family: variants
  - given: was responsible for our initial
    family: codebase
  - given: >-
      and efficient inference and visualizations. Lukasz and Aidan spent countless long days
      designing various parts of and implementing
    family: tensor2tensor
  - given: replacing our earlier
    family: codebase
  - given: greatly improving results and massively accelerating our
    family: research.
date: '2026-03-02'
url: https://arxiv.org/abs/1706.03762
pdf: https://arxiv.org/pdf/1706.03762
arxiv_id: '1706.03762'
archive: arxiv
archive_url: https://arxiv.org/abs/1706.03762
source: arxiv-pipeline
pipeline_version: '2.0'
tags:
  - auto-imported
  - arxiv-pipeline
abstract: >-
  The dominant sequence transduction models are based on complex recurrent or convolutional neural
  networks that include an encoder and a decoder. The best performing models also connect the
  encoder and decoder through an attention mechanism. We propose a new simple network architecture,
  the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions
  entirely. Experiments on two machine translation tasks show these models to be superior in quality
  while being  mo
files:
  - name: paper.html
    format: html
    description: Rendered with KaTeX math and extracted figures
    primary: true
variable_count: 0
equation_count: 16
reference_count: 40
sections: 10
figures: 6
