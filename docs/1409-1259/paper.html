<h1>On the Properties of Neural Machine Translation: Encoder--Decoder Approaches</h1>

<p class="authors">Kyunghyun Cho~~~~~~~~~Bart van Merri\"enboer Universit\'e de Montr\'eal</p>

<h2>Abstract</h2>
<p>Neural machine translation is a relatively new approach to statistical
    machine translation based purely on neural networks. The neural machine
    translation models often consist of an encoder and a decoder. The encoder
    extracts a fixed-length representation from a variable-length input
    sentence, and the decoder generates a correct translation from this
    representation. In this paper, we focus on analyzing the properties of the
    neural machine translation using two models; RNN Encoder--Decoder and a
    newly proposed gated recursive convolutional neural network. We show that
    the neural machine translation performs relatively well on short sentences
    without unknown words, but its performance degrades rapidly as the length of
    the sentence and the number of unknown words increase. Furthermore, we find
    that the proposed gated recursive convolutional network learns a grammatical
    structure of a sentence automatically.</p>

<hr>

<h2>1. Introduction</h2>

<p>A new approach for statistical machine translation based purely on neural
networks has recently been proposed~<sup class="ref-badge" data-ref="7" data-title="2013.">7</sup><sup class="ref-badge" data-ref="11" data-title="2014.">11</sup>. This
new approach, which we refer to as <em>neural machine translation</em>, is inspired
by the recent trend of deep representational learning. All the neural network
models used in <sup class="ref-badge" data-ref="7" data-title="2013.">7</sup><sup class="ref-badge" data-ref="11" data-title="2014.">11</sup><sup class="ref-badge" data-ref="3" data-title="Schwenk, and Yoshua Bengio.">3</sup> consist of an
encoder and a decoder. The encoder extracts a fixed-length vector representation
from a variable-length input sentence, and from this representation the decoder
generates a correct, variable-length target translation.</p>

<p>The emergence of the neural machine translation is highly significant, both
practically and theoretically. Neural machine translation models require only a
fraction of the memory needed by traditional statistical machine translation
(SMT) models. The models we trained for this paper require only 500MB of memory
in total.  This stands in stark contrast with existing SMT systems, which often
require tens of gigabytes of memory. This makes the neural machine translation
appealing in practice. Furthermore, unlike conventional translation systems,
each and every component of the neural translation model is trained jointly to
maximize the translation performance.</p>

<p>As this approach is relatively new, there has not been much work on analyzing
the properties and behavior of these models. For instance: What are the
properties of sentences on which this approach performs better? How does the
choice of source/target vocabulary affect the performance? In which cases does
the neural machine translation fail?</p>

<p>It is crucial to understand the properties and behavior of this new neural
machine translation approach in order to determine future research directions.
Also, understanding the weaknesses and strengths of neural machine translation
might lead to better ways of integrating SMT and neural machine  translation
systems.</p>

<p>In this paper, we analyze two neural machine translation models. One of them is
the RNN Encoder--Decoder that was proposed recently in <sup class="ref-badge" data-ref="3" data-title="Schwenk, and Yoshua Bengio.">3</sup>. The other
model replaces the encoder in the RNN Encoder--Decoder model with a novel neural
network, which we call a <em>gated recursive convolutional neural network</em>
(grConv). We evaluate these two models on the task of translation from French to
English.</p>

<p>Our analysis shows that the performance of the neural machine translation model
degrades quickly as the length of a source sentence increases. Furthermore, we
find that the vocabulary size has a high impact on the translation performance.
Nonetheless, qualitatively we find that the both models are able to generate
correct translations most of the time. Furthermore, the newly proposed grConv
model is able to learn, without supervision, a kind of syntactic structure over
the source language.</p>

<hr>

<h2>2. Neural Networks for Variable-Length Sequences</h2>

<p>In this section, we describe two types of neural networks that are able to
process variable-length sequences. These are the recurrent neural network
and the proposed gated recursive convolutional neural network.</p>

### Recurrent Neural Network with Gated Hidden Neurons
<span id="sec:rnn_gated" class="label-anchor"></span>

<p>A recurrent neural network (RNN, Fig.~<a href="#fig:rnn_unit" class="cross-ref">Fig. 1</a> (a)) works on a variable-length sequence $x=(\vx_1,
\vx_2, \cdots, \vx_T)<span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi><mi>y</mi><mi>m</mi><mi>a</mi><mi>i</mi><mi>n</mi><mi>t</mi><mi>a</mi><mi>i</mi><mi>n</mi><mi>i</mi><mi>n</mi><mi>g</mi><mi>a</mi><mi>h</mi><mi>i</mi><mi>d</mi><mi>d</mi><mi>e</mi><mi>n</mi><mi>s</mi><mi>t</mi><mi>a</mi><mi>t</mi><mi>e</mi></mrow><annotation encoding="application/x-tex">by maintaining a hidden state</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="var" data-var="b" data-desc="Bias vector" style="--var-color: #ffd700"><span class="mord mathnormal">b</span></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mord mathnormal">main</span><span class="mord mathnormal">t</span><span class="mord mathnormal">ainin</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal">ahi</span><span class="mord mathnormal">dd</span><span class="mord mathnormal">e</span><span class="var" data-var="n" data-desc="Sequence length" style="--var-color: #c678dd"><span class="mord mathnormal">n</span></span><span class="mord mathnormal">s</span><span class="mord mathnormal">t</span><span class="mord mathnormal">a</span><span class="mord mathnormal">t</span><span class="mord mathnormal">e</span></span></span></span></span>\vh$ over time. At each
timestep <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6151em;"></span><span class="mord mathnormal">t</span></span></span></span></span>, the hidden state <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi mathvariant="bold">h</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">\vh^{(t)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.888em;"></span><span class="mord"><span class="mord mathbf">h</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.888em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">t</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span></span></span> is updated by</p>

<div class="math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msup><mi mathvariant="bold">h</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo>=</mo><mi>f</mi><mrow><mo fence="true">(</mo><msup><mi mathvariant="bold">h</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo separator="true">,</mo><msub><mi mathvariant="bold">x</mi><mi>t</mi></msub><mo fence="true">)</mo></mrow><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">\vh^{(t)} = f\left( \vh^{(t-1)}, \vx_t \right),</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.938em;"></span><span class="mord"><span class="mord mathbf">h</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.938em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">t</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.8em;vertical-align:-0.65em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size2">(</span></span><span class="mord"><span class="mord mathbf">h</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.938em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathbf">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size2">)</span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span></span></span></span></span></div>

<p>where <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span></span></span></span> is an activation function. Often <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span></span></span></span> is as simple as performing a
linear transformation on the input vectors, summing them, and applying an
element-wise logistic sigmoid function.</p>

<p>An RNN can be used effectively to learn a distribution over a variable-length
sequence by learning the distribution over the next input $p(\vx_{t+1} \mid
\vx_{t}, \cdots, \vx_{1})$. For instance, in the case of a sequence of
<span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn></mrow><annotation encoding="application/x-tex">1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1</span></span></span></span></span>-of-<span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="var" data-var="K" data-desc="Key matrix — packed keys (shape: seq_len × d_k)" style="--var-color: #58a6ff"><span class="mord mathnormal" style="margin-right:0.07153em;">K</span></span></span></span></span></span> vectors, the distribution can be learned by an RNN which has as an
output</p>

<div class="math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>p</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mi>t</mi><mo separator="true">,</mo><mi>j</mi></mrow></msub><mo>=</mo><mn>1</mn><mo>∣</mo><msub><mi mathvariant="bold">x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi mathvariant="bold">x</mi><mn>1</mn></msub><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi>exp</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><msub><mi mathvariant="bold">w</mi><mi>j</mi></msub><msub><mi mathvariant="bold">h</mi><mrow><mo fence="true">&lt;</mo><mi>t</mi><mo fence="true">&gt;</mo></mrow></msub><mo fence="true">)</mo></mrow></mrow><mrow><munderover><mo>∑</mo><mrow><msup><mi>j</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><mi>exp</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><msub><mi mathvariant="bold">w</mi><msup><mi>j</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></msub><msub><mi mathvariant="bold">h</mi><mrow><mo fence="true">&lt;</mo><mi>t</mi><mo fence="true">&gt;</mo></mrow></msub><mo fence="true">)</mo></mrow></mrow></mfrac><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">p(x_{t,j} = 1 \mid \vx_{t-1}, \dots, \vx_1) = \frac{\exp \left(
        \vw_j \vh_{\qt{t}}\right) } {\sum_{j&#x27;=1}^{K} \exp \left( \vw_{j&#x27;}
        \vh_{\qt{t}}\right) },</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em;"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord"><span class="var" data-var="x" data-desc="Input tensor" style="--var-color: #98c379"><span class="mord mathnormal">x</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathbf">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathbf">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.9022em;vertical-align:-1.307em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5952em;"><span style="top:-2.1288em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9812em;"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6828em;"><span style="top:-2.786em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4358em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop">exp</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size1">(</span></span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.328em;"><span style="top:-2.55em;margin-left:-0.016em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6828em;"><span style="top:-2.786em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathbf">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.5198em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="minner mtight"><span class="mopen mtight delimcenter" style="top:0em;"><span class="mtight">⟨</span></span><span class="mord mathnormal mtight">t</span><span class="mclose mtight delimcenter" style="top:0em;"><span class="mtight">⟩</span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3552em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size1">)</span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.7452em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mop">exp</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size1">(</span></span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.016em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathbf">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.5198em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="minner mtight"><span class="mopen mtight delimcenter" style="top:0em;"><span class="mtight">⟨</span></span><span class="mord mathnormal mtight">t</span><span class="mclose mtight delimcenter" style="top:0em;"><span class="mtight">⟩</span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3552em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size1">)</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.307em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mpunct">,</span></span></span></span></span></div>

<p>for all possible symbols <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>j</mi><mo>=</mo><mn>1</mn><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><mi>K</mi></mrow><annotation encoding="application/x-tex">j=1,\dots,K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.854em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05724em;">j</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="var" data-var="K" data-desc="Key matrix — packed keys (shape: seq_len × d_k)" style="--var-color: #58a6ff"><span class="mord mathnormal" style="margin-right:0.07153em;">K</span></span></span></span></span></span>, where <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">w</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\vw_j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7305em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.016em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span></span> are the rows of a
weight matrix <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">W</mi></mrow><annotation encoding="application/x-tex">\mW</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6861em;"></span><span class="mord mathbf" style="margin-right:0.01597em;">W</span></span></span></span></span>. This results in the joint distribution</p>

<div class="math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>p</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><munderover><mo>∏</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></munderover><mi>p</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo>∣</mo><msub><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>x</mi><mn>1</mn></msub><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">p(x) = \prod_{t=1}^T p(x_t \mid x_{t-1}, \dots, x_1).</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="var" data-var="x" data-desc="Input tensor" style="--var-color: #98c379"><span class="mord mathnormal">x</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:3.0954em;vertical-align:-1.2671em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283em;"><span style="top:-1.8829em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∏</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2671em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord"><span class="var" data-var="x" data-desc="Input tensor" style="--var-color: #98c379"><span class="mord mathnormal">x</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="var" data-var="x" data-desc="Input tensor" style="--var-color: #98c379"><span class="mord mathnormal">x</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="var" data-var="x" data-desc="Input tensor" style="--var-color: #98c379"><span class="mord mathnormal">x</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord">.</span></span></span></span></span></div>

<p>Recently, in <sup class="ref-badge" data-ref="3" data-title="Schwenk, and Yoshua Bengio.">3</sup> a new activation function for RNNs was proposed.
The new activation function augments the usual logistic sigmoid activation
function with two gating units called reset, <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">r</mi></mrow><annotation encoding="application/x-tex">\vr</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4444em;"></span><span class="mord mathbf">r</span></span></span></span></span>, and update, <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">z</mi></mrow><annotation encoding="application/x-tex">\vz</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4444em;"></span><span class="mord mathbf">z</span></span></span></span></span>, gates.
Each gate depends on the previous hidden state <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi mathvariant="bold">h</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">\vh^{(t-1)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.888em;"></span><span class="mord"><span class="mord mathbf">h</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.888em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span></span></span>, and the current
input <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">x</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\vx_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5944em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathbf">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span> controls the flow of information. This is reminiscent of long
short-term memory (LSTM) units~<sup class="ref-badge" data-ref="6" data-title="1997.">6</sup>. For details about this
unit, we refer the reader to <sup class="ref-badge" data-ref="3" data-title="Schwenk, and Yoshua Bengio.">3</sup> and Fig.~<a href="#fig:rnn_unit" class="cross-ref">Fig. 1</a> (b). For
the remainder of this paper, we always use this new activation function.</p>

### Gated Recursive Convolutional Neural Network
<span id="sec:grconv" class="label-anchor"></span>

<p>Besides RNNs, another natural approach to dealing with variable-length sequences
is to use a recursive convolutional neural network where the parameters at each
level are shared through the whole network (see Fig.~<a href="#fig:rconv_unit" class="cross-ref">Fig. 2</a> (a)).
In this section, we introduce a binary convolutional neural network whose
weights are recursively applied to the input sequence until it outputs a single
fixed-length vector. In addition to a usual convolutional architecture, we
propose to use the previously mentioned gating mechanism, which allows the
recursive network to learn the structure of the source sentences on the fly.</p>

<p>Let <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>=</mo><mo stretchy="false">(</mo><msub><mi mathvariant="bold">x</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi mathvariant="bold">x</mi><mn>2</mn></msub><mo separator="true">,</mo><mo>⋯</mo><mtext> </mtext><mo separator="true">,</mo><msub><mi mathvariant="bold">x</mi><mi>T</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">x=(\vx_1, \vx_2, \cdots, \vx_T)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="var" data-var="x" data-desc="Input tensor" style="--var-color: #98c379"><span class="mord mathnormal">x</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathbf">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathbf">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">⋯</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathbf">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span> be an input sequence, where $\vx_t \in
\RR^d$.  The proposed gated recursive convolutional neural network (grConv)
consists of four weight matrices <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi mathvariant="bold">W</mi><mi>l</mi></msup></mrow><annotation encoding="application/x-tex">\mW^l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8491em;"></span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span></span></span></span></span></span></span></span></span>, <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi mathvariant="bold">W</mi><mi>r</mi></msup></mrow><annotation encoding="application/x-tex">\mW^r</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6861em;"></span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span></span></span></span></span></span></span></span></span></span></span></span>, <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi mathvariant="bold">G</mi><mi>l</mi></msup></mrow><annotation encoding="application/x-tex">\mG^l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8491em;"></span><span class="mord"><span class="mord mathbf">G</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span></span></span></span></span></span></span></span></span> and <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi mathvariant="bold">G</mi><mi>r</mi></msup></mrow><annotation encoding="application/x-tex">\mG^r</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6861em;"></span><span class="mord"><span class="mord mathbf">G</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span></span></span></span></span></span></span></span></span></span></span></span>. At each
recursion level <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo>∈</mo><mrow><mo fence="true">[</mo><mn>1</mn><mo separator="true">,</mo><mi>T</mi><mo>−</mo><mn>1</mn><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">t \in \left[ 1, T-1\right]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6542em;vertical-align:-0.0391em;"></span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">[</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="var" data-var="T" data-desc="Transpose" style="--var-color: #f0a050"><span class="mord mathnormal" style="margin-right:0.13889em;">T</span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord">1</span><span class="mclose delimcenter" style="top:0em;">]</span></span></span></span></span></span>, the activation of the <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>j</mi></mrow><annotation encoding="application/x-tex">j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.854em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05724em;">j</span></span></span></span></span>-th
hidden unit <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>h</mi><mi>j</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow><annotation encoding="application/x-tex">h^{(t)}_j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.4578em;vertical-align:-0.413em;"></span><span class="mord"><span class="var" data-var="h" data-desc="Number of parallel attention heads (h = 8)" style="--var-color: #ff7b72"><span class="mord mathnormal">h</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.4231em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span><span style="top:-3.2198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">t</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.413em;"><span></span></span></span></span></span></span></span></span></span></span> is computed by</p>

<div class="math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msubsup><mi>h</mi><mi>j</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>=</mo><msub><mi>ω</mi><mi>c</mi></msub><msubsup><mover accent="true"><mi>h</mi><mo>~</mo></mover><mi>j</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>+</mo><msub><mi>ω</mi><mi>l</mi></msub><msubsup><mi>h</mi><mrow><mi>j</mi><mo>−</mo><mn>1</mn></mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup><mo>+</mo><msub><mi>ω</mi><mi>r</mi></msub><msubsup><mi>h</mi><mi>j</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">
    h^{(t)}_j = \omega_c \tilde{h}^{(t)}_j + \omega_l h^{(t-1)}_{j-1} + \omega_r
    h^{(t-1)}_j,</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.4578em;vertical-align:-0.413em;"></span><span class="mord"><span class="var" data-var="h" data-desc="Number of parallel attention heads (h = 8)" style="--var-color: #ff7b72"><span class="mord mathnormal">h</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.4231em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span><span style="top:-3.2198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">t</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.413em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.4578em;vertical-align:-0.413em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">ω</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9313em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="var" data-var="h" data-desc="Number of parallel attention heads (h = 8)" style="--var-color: #ff7b72"><span class="mord mathnormal">h</span></span></span><span style="top:-3.6134em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.25em;"><span class="mord">~</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.4231em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span><span style="top:-3.2198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">t</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.413em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.4578em;vertical-align:-0.413em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">ω</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="var" data-var="h" data-desc="Number of parallel attention heads (h = 8)" style="--var-color: #ff7b72"><span class="mord mathnormal">h</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.4231em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.413em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.4578em;vertical-align:-0.413em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">ω</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="var" data-var="h" data-desc="Number of parallel attention heads (h = 8)" style="--var-color: #ff7b72"><span class="mord mathnormal">h</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.4231em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span><span style="top:-3.2198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.413em;"><span></span></span></span></span></span></span><span class="mpunct">,</span></span></span></span></span></div>

<p>where <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>ω</mi><mi>c</mi></msub></mrow><annotation encoding="application/x-tex">\omega_c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">ω</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span>, <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>ω</mi><mi>l</mi></msub></mrow><annotation encoding="application/x-tex">\omega_l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">ω</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span> and <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>ω</mi><mi>r</mi></msub></mrow><annotation encoding="application/x-tex">\omega_r</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">ω</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span> are the values of a gater that sum
to <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn></mrow><annotation encoding="application/x-tex">1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1</span></span></span></span></span>. The hidden unit is initialized as</p>

<div class="math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msubsup><mi>h</mi><mi>j</mi><mrow><mo stretchy="false">(</mo><mn>0</mn><mo stretchy="false">)</mo></mrow></msubsup><mo>=</mo><mi mathvariant="bold">U</mi><msub><mi mathvariant="bold">x</mi><mi>j</mi></msub><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">h^{(0)}_j = \mU \vx_j,</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.4578em;vertical-align:-0.413em;"></span><span class="mord"><span class="var" data-var="h" data-desc="Number of parallel attention heads (h = 8)" style="--var-color: #ff7b72"><span class="mord mathnormal">h</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.4231em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span><span style="top:-3.2198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">0</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.413em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.9722em;vertical-align:-0.2861em;"></span><span class="mord mathbf">U</span><span class="mord"><span class="mord mathbf">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mpunct">,</span></span></span></span></span></div>

<p>where <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">U</mi></mrow><annotation encoding="application/x-tex">\mU</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6861em;"></span><span class="mord mathbf">U</span></span></span></span></span> projects the input into a hidden space.</p>

<p>The new activation <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mover accent="true"><mi>h</mi><mo>~</mo></mover><mi>j</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow><annotation encoding="application/x-tex">\tilde{h}^{(t)}_j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.4578em;vertical-align:-0.413em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9313em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="var" data-var="h" data-desc="Number of parallel attention heads (h = 8)" style="--var-color: #ff7b72"><span class="mord mathnormal">h</span></span></span><span style="top:-3.6134em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.25em;"><span class="mord">~</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.4231em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span><span style="top:-3.2198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">t</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.413em;"><span></span></span></span></span></span></span></span></span></span></span> is computed as usual:</p>

<div class="math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msubsup><mover accent="true"><mi>h</mi><mo>~</mo></mover><mi>j</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>=</mo><mi>ϕ</mi><mrow><mo fence="true">(</mo><msup><mi mathvariant="bold">W</mi><mi>l</mi></msup><msubsup><mi>h</mi><mrow><mi>j</mi><mo>−</mo><mn>1</mn></mrow><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>+</mo><msup><mi mathvariant="bold">W</mi><mi>r</mi></msup><msubsup><mi>h</mi><mi>j</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msubsup><mo fence="true">)</mo></mrow><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">\tilde{h}^{(t)}_j = \phi\left( \mW^l h^{(t)}_{j-1} + \mW^r h^{(t)}_{j}
    \right),</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.4578em;vertical-align:-0.413em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9313em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="var" data-var="h" data-desc="Number of parallel attention heads (h = 8)" style="--var-color: #ff7b72"><span class="mord mathnormal">h</span></span></span><span style="top:-3.6134em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.25em;"><span class="mord">~</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.4231em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span><span style="top:-3.2198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">t</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.413em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.8em;vertical-align:-0.65em;"></span><span class="mord mathnormal">ϕ</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size2">(</span></span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8991em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span></span></span></span></span><span class="mord"><span class="var" data-var="h" data-desc="Number of parallel attention heads (h = 8)" style="--var-color: #ff7b72"><span class="mord mathnormal">h</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.4231em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">t</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.413em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7144em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span></span></span></span></span></span></span></span><span class="mord"><span class="var" data-var="h" data-desc="Number of parallel attention heads (h = 8)" style="--var-color: #ff7b72"><span class="mord mathnormal">h</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.4231em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span style="top:-3.2198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">t</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.413em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size2">)</span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span></span></span></span></span></div>

<p>where <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϕ</mi></mrow><annotation encoding="application/x-tex">\phi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">ϕ</span></span></span></span></span> is an element-wise nonlinearity.</p>

<p>The gating coefficients <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ω</mi></mrow><annotation encoding="application/x-tex">\omega</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">ω</span></span></span></span></span>'s are computed by</p>

<div class="math-display"><span class="katex-error" title="ParseError: KaTeX parse error: Expected &amp; or \\ or \cr or \end at end of input: …       \omega_c" style="color:#cc0000">\left[ 
        \begin{array}{c}
            \omega_c</span></div>
<div class="math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>ω</mi><mi>l</mi></msub></mrow><annotation encoding="application/x-tex">\omega_l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">ω</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span></div>
<div class="math-display"><span class="katex-error" title="ParseError: KaTeX parse error: Expected &#x27;EOF&#x27;, got &#x27;\end&#x27; at position 18: …mega_r
        \̲e̲n̲d̲{array}
    \ri…" style="color:#cc0000">\omega_r
        \end{array}
    \right] = \frac{1}{Z}
    \exp\left( \mG^l h^{(t)}_{j-1} + \mG^r h^{(t)}_{j}
    \right),</span></div>

<p>where <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi mathvariant="bold">G</mi><mi>l</mi></msup><mo separator="true">,</mo><msup><mi mathvariant="bold">G</mi><mi>r</mi></msup><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mn>3</mn><mo>×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\mG^l, \mG^r \in \RR^{3 \times d}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0435em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathbf">G</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathbf">G</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8491em;"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight">d</span></span></span></span></span></span></span></span></span></span></span></span></span> and 
\[
    Z = \sum_{k=1}^3 \left[\exp\left( \mG^l h^{(t)}_{j-1} + \mG^r h^{(t)}_{j} \right)\right]_k.
\]</p>

<p>According to this activation, one can think of the activation of a single node
at recursion level <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6151em;"></span><span class="mord mathnormal">t</span></span></span></span></span> as a choice between either a new activation computed from
both left and right children, the activation from the left child, or the
activation from the right child. This choice allows the overall structure of the
recursive convolution to change adaptively with respect to an input sample. See
Fig.~<a href="#fig:rconv_unit" class="cross-ref">Fig. 2</a> (b) for an illustration.</p>

<p>In this respect, we may even consider the proposed grConv as doing a kind of
unsupervised parsing. If we consider the case where the gating unit makes a
hard decision, i.e., <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ω</mi></mrow><annotation encoding="application/x-tex">\omega</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">ω</span></span></span></span></span> follows an 1-of-K coding, it is easy to see that
the network adapts to the input and forms a tree-like structure (See
Fig.~<a href="#fig:rconv_unit" class="cross-ref">Fig. 2</a> (c--d)). However, we leave the further investigation
of the structure learned by this model for future research.</p>

<hr>

<h2>3. Purely Neural Machine Translation</h2>

### Encoder--Decoder Approach

<p>The task of translation can be understood from the perspective of machine learning
as learning the conditional distribution <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy="false">(</mo><mi>f</mi><mo>∣</mo><mi>e</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p(f \mid e)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">e</span><span class="mclose">)</span></span></span></span></span> of a target sentence
(translation) <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span></span></span></span> given a source sentence <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>e</mi></mrow><annotation encoding="application/x-tex">e</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">e</span></span></span></span></span>. Once the conditional distribution
is learned by a model, one can use the model to directly sample a target
sentence given a source sentence, either by actual sampling or by using a
(approximate) search algorithm to find the maximum of the distribution.</p>

<p>A number of recent papers have proposed to use neural networks to directly learn
the conditional distribution from a bilingual, parallel
corpus~<sup class="ref-badge" data-ref="7" data-title="2013.">7</sup><sup class="ref-badge" data-ref="3" data-title="Schwenk, and Yoshua Bengio.">3</sup><sup class="ref-badge" data-ref="11" data-title="2014.">11</sup>. For instance, the authors
of <sup class="ref-badge" data-ref="7" data-title="2013.">7</sup> proposed an approach involving a convolutional
<span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="var" data-var="n" data-desc="Sequence length" style="--var-color: #c678dd"><span class="mord mathnormal">n</span></span></span></span></span></span>-gram model to extract a fixed-length vector of a source sentence which is
decoded with an inverse convolutional <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="var" data-var="n" data-desc="Sequence length" style="--var-color: #c678dd"><span class="mord mathnormal">n</span></span></span></span></span></span>-gram model augmented with an RNN. In
<sup class="ref-badge" data-ref="11" data-title="2014.">11</sup>, an RNN with LSTM units was used to encode a source
sentence and starting from the last hidden state, to decode a target sentence.
Similarly, the authors of <sup class="ref-badge" data-ref="3" data-title="Schwenk, and Yoshua Bengio.">3</sup> proposed to use an RNN to encode and
decode a pair of source and target phrases.</p>

<p>At the core of all these recent works lies an encoder--decoder architecture (see
Fig.~<a href="#fig:encode_decode" class="cross-ref">Fig. 3</a>). The encoder processes a variable-length input
(source sentence) and builds a fixed-length vector representation (denoted as
<span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">z</mi></mrow><annotation encoding="application/x-tex">\vz</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4444em;"></span><span class="mord mathbf">z</span></span></span></span></span> in Fig.~<a href="#fig:encode_decode" class="cross-ref">Fig. 3</a>). Conditioned on the encoded
representation, the decoder generates a variable-length sequence (target
sentence).</p>

<p>Before <sup class="ref-badge" data-ref="11" data-title="2014.">11</sup> this encoder--decoder approach was used mainly as
a part of the existing statistical machine translation (SMT) system. This
approach was used to re-rank the <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="var" data-var="n" data-desc="Sequence length" style="--var-color: #c678dd"><span class="mord mathnormal">n</span></span></span></span></span></span>-best list generated by the SMT system in
<sup class="ref-badge" data-ref="7" data-title="2013.">7</sup>, and the authors of <sup class="ref-badge" data-ref="3" data-title="Schwenk, and Yoshua Bengio.">3</sup> used this approach
to provide an additional score for the existing phrase table.</p>

<p>In this paper, we concentrate on analyzing the direct translation performance,
as in <sup class="ref-badge" data-ref="11" data-title="2014.">11</sup>, with two model configurations. In both models, we
use an RNN with the gated hidden unit~<sup class="ref-badge" data-ref="3" data-title="Schwenk, and Yoshua Bengio.">3</sup>, as this is one of the only
options that does not require a non-trivial way to determine the target length.
The first model will use the same RNN with the gated hidden unit as an encoder,
as in <sup class="ref-badge" data-ref="3" data-title="Schwenk, and Yoshua Bengio.">3</sup>, and the second one will use the proposed gated recursive
convolutional neural network (grConv). We aim to understand the inductive bias
of the encoder--decoder approach on the translation performance measured by
BLEU.</p>

<hr>

<h2>4. Experiment Settings</h2>

### Dataset

<p>We evaluate the encoder--decoder models on the task of English-to-French
translation. We use the bilingual, parallel corpus which is a set of 348M
selected by the method in <sup class="ref-badge" data-ref="1" data-title="2011.">1</sup> from a combination of Europarl (61M
words), news commentary (5.5M), UN (421M) and two crawled corpora of 90M and
780M words respectively. We did not use
separate monolingual data. The performance of the neural machien translation
models was measured on the news-test2012, news-test2013 and news-test2014 sets
( 3000 lines each). When comparing to the SMT system, we use news-test2012 and
news-test2013 as our development set for tuning the SMT system, and
news-test2014 as our test set.</p>

<p>Among all the sentence pairs in the prepared parallel corpus, for reasons of
computational efficiency  we only use the pairs where both English and French
sentences are at most 30 words long to train neural networks. Furthermore, we
use only the 30,000 most frequent words for both English and French. All the
other rare words are considered unknown and are mapped to a special token
(<span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo fence="true">[</mo><mtext>UNK</mtext><mo fence="true">]</mo></mrow><annotation encoding="application/x-tex">\left[ \text{UNK} \right]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">[</span><span class="mord text"><span class="mord">UNK</span></span><span class="mclose delimcenter" style="top:0em;">]</span></span></span></span></span></span>).</p>

### Models

<p>We train two models: The RNN Encoder--Decoder (RNNenc)<sup class="ref-badge" data-ref="3" data-title="Schwenk, and Yoshua Bengio.">3</sup> and the
newly proposed gated recursive convolutional neural network (grConv). Note that
both models use an RNN with gated hidden units as a decoder (see
Sec.~<a href="#sec:rnn_gated" class="cross-ref">Section 1</a>).</p>

<p>We use minibatch stochastic gradient descent with AdaDelta~<sup class="ref-badge" data-ref="12" data-title="2012." data-arxiv-id="1212.5701">12</sup> to
train our two models. We initialize the square weight matrix (transition matrix)
as an orthogonal matrix with its spectral radius set to <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn></mrow><annotation encoding="application/x-tex">1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1</span></span></span></span></span> in the case of the
RNNenc and <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0.4</mn></mrow><annotation encoding="application/x-tex">0.4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">0.4</span></span></span></span></span> in the case of the grConv. <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>tanh</mi><mo>⁡</mo></mrow><annotation encoding="application/x-tex">\tanh</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mop">tanh</span></span></span></span></span> and a rectifier
(<span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>max</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\max(0,x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">max</span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="var" data-var="x" data-desc="Input tensor" style="--var-color: #98c379"><span class="mord mathnormal">x</span></span><span class="mclose">)</span></span></span></span></span>) are used as the element-wise nonlinear functions for the RNNenc
and grConv respectively.</p>

<p>The grConv has 2000 hidden neurons, whereas the RNNenc has 1000 hidden
neurons. The word embeddings are 620-dimensional in both cases.
Both models were trained for approximately 110 hours, which is equivalent to
296,144 updates and 846,322 updates for the grConv and RNNenc,
respectively.</p>

<table class="article-table">
  <caption>BLEU scores computed on the development and test sets. The top
        three rows show the scores on all the sentences, and the bottom three
        rows on the sentences having no unknown words. (<span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>⋆</mo></mrow><annotation encoding="application/x-tex">\star</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4653em;"></span><span class="mord">⋆</span></span></span></span></span>) The result
        reported in <sup class="ref-badge" data-ref="3" data-title="Schwenk, and Yoshua Bengio.">3</sup> where the RNNenc was used to score phrase
        pairs in the phrase table. (<span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>∘</mo></mrow><annotation encoding="application/x-tex">\circ</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4445em;"></span><span class="mord">∘</span></span></span></span></span>) The result reported in
        <sup class="ref-badge" data-ref="11" data-title="2014.">11</sup> where an encoder--decoder with LSTM units was used
    to re-rank the <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="var" data-var="n" data-desc="Sequence length" style="--var-color: #c678dd"><span class="mord mathnormal">n</span></span></span></span></span></span>-best list generated by Moses.</caption>
  <thead><tr>
    <th></th>
    <th>Model</th>
    <th>Development</th>
    <th>Test</th>
  </tr></thead>
  <tbody>
  <tr>
    <td>\multirow{5}{*}{\rotatebox[origin=c]{90}{All}}</td>
    <td>RNNenc</td>
    <td>13.15</td>
    <td>13.92</td>
  </tr>
  <tr>
    <td></td>
    <td>grConv</td>
    <td>9.97</td>
    <td>9.97</td>
  </tr>
  <tr>
    <td></td>
    <td>Moses</td>
    <td>30.64</td>
    <td>33.30</td>
  </tr>
  <tr>
    <td></td>
    <td>Moses+RNNenc<span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mo>⋆</mo></msup></mrow><annotation encoding="application/x-tex">^\star</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6887em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6887em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">⋆</span></span></span></span></span></span></span></span></span></span></span></span></td>
    <td>31.48</td>
    <td>34.64</td>
  </tr>
  <tr>
    <td></td>
    <td>Moses+LSTM<span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mo>∘</mo></msup></mrow><annotation encoding="application/x-tex">^\circ</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6741em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6741em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">∘</span></span></span></span></span></span></span></span></span></span></span></span></td>
    <td>32</td>
    <td>35.65</td>
  </tr>
  <tr>
    <td>\multirow{3}{*}{\rotatebox[origin=c]{90}{No UNK}}</td>
    <td>RNNenc</td>
    <td>21.01</td>
    <td>23.45</td>
  </tr>
  <tr>
    <td></td>
    <td>grConv</td>
    <td>17.19</td>
    <td>18.22</td>
  </tr>
  <tr>
    <td></td>
    <td>Moses</td>
    <td>32.77</td>
    <td>35.63</td>
  </tr>
  </tbody>
</table>

#### Translation using Beam-Search

<p>We use a basic form of beam-search to find a translation that maximizes the
conditional probability given by a specific model (in this case, either the
RNNenc or the grConv). At each time step of the decoder, we keep the <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi></mrow><annotation encoding="application/x-tex">s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">s</span></span></span></span></span>
translation candidates with the highest log-probability, where <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mo>=</mo><mn>10</mn></mrow><annotation encoding="application/x-tex">s=10</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">s</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">10</span></span></span></span></span> is the
beam-width.  During the beam-search, we exclude any hypothesis that includes an
unknown word.  For each end-of-sequence symbol that is selected among the
highest scoring candidates the beam-width is reduced by one, until the
beam-width reaches zero.</p>

<p>The beam-search to (approximately) find a sequence of maximum log-probability
under RNN was proposed and used successfully in <sup class="ref-badge" data-ref="4" data-title="2012.">4</sup> and
<sup class="ref-badge" data-ref="2" data-title="2013.">2</sup>.  Recently, the authors of <sup class="ref-badge" data-ref="11" data-title="2014.">11</sup> found this
approach to be effective in purely neural machine translation based on LSTM
units.</p>

<p>When we use the beam-search to find the <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span></span></span></span></span> best translations, we do not use a
usual log-probability but one normalized with respect to the length of the
translation.  This prevents the RNN decoder from favoring shorter translations,
behavior which was observed earlier in, e.g.,~<sup class="ref-badge" data-ref="5" data-title="2013.">5</sup>.</p>

<table class="article-table">
  <caption>The sample translations along with the source sentences and the reference translations.</caption>
  <thead><tr>
    <th>}
Source</th>
    <th>She explained her new position of foreign affairs and security policy
representative as a reply to a question: "Who is the European Union? Which phone
number should I call?"; i.e. as an important step to unification and better
clarity of Union's policy towards countries such as China or India.</th>
  </tr></thead>
  <tbody>
  <tr>
    <td>Reference</td>
    <td>Elle a expliqué le nouveau poste de la Haute représentante pour les
affaires étrangères et la politique de défense dans le cadre d'une réponse à la
question: "Qui est qui à l'Union européenne?" "A quel numéro de téléphone
dois-je appeler?", donc comme un pas important vers l'unicité et une plus grande
lisibilité de la politique de l'Union face aux états, comme est la Chine ou bien
l'Inde.</td>
  </tr>
  <tr>
    <td>RNNEnc</td>
    <td>Elle a décrit sa position en matière de politique étrangère et de
sécurité ainsi que la politique de l'Union européenne en matière de gouvernance
et de démocratie .</td>
  </tr>
  <tr>
    <td>grConv</td>
    <td>Elle a expliqué sa nouvelle politique étrangère et de sécurité en
réponse à un certain nombre de questions : "Qu'est-ce que l'Union européenne ? "
.</td>
  </tr>
  <tr>
    <td>Moses</td>
    <td>Elle a expliqué son nouveau poste des affaires étrangères et la
politique de sécurité représentant en réponse à une question: "Qui est l'Union
européenne? Quel numéro de téléphone dois-je appeler?"; c'est comme une étape
importante de l'unification et une meilleure lisibilité de la politique de
l'Union à des pays comme la Chine ou l'Inde .</td>
  </tr>
  <tr>
    <td>\multicolumn{2}{c}{}</td>
  </tr>
  <tr>
    <td>Source</td>
    <td>The investigation should be complete by the end of the year when the
findings will be presented to Deutsche Bank's board of managing directors - with
recommendations for action.</td>
  </tr>
  <tr>
    <td>Reference</td>
    <td>L'examen doit être terminé d'ici la fin de l'année, ensuite les
résultats du conseil d'administration de la Deutsche Bank doivent être présentés
- avec recommandation, d' habitude.</td>
  </tr>
  <tr>
    <td>RNNEnc</td>
    <td>L'étude devrait être terminée à la fin de l' année, lorsque les
conclusions seront présentées au conseil d'administration de la Deutsche Bank,
conseil d'association avec des mesures.</td>
  </tr>
  <tr>
    <td>grConv</td>
    <td>L'enquête devrait être terminée à la fin de l'année où les conclusions
seront présentées par le conseil d'administration de la BCE à la direction des
recommandations.</td>
  </tr>
  <tr>
    <td>Moses</td>
    <td>L'enquête devrait être terminé d'ici la fin de l'année lorsque les
résultats seront présentés à la Deutsche Bank conseil des directeurs généraux -
avec des recommandations .</td>
  </tr>
  <tr>
    <td>\multicolumn{2}{c}{}</td>
  </tr>
  <tr>
    <td>Source</td>
    <td>And there are thorny mechanical questions that must be resolved during
that time, like how to balance the state's mandate of "adequate access" to
licensed marijuana with its prohibitions on cannabis businesses within 1,000
feet of a school, park, playground or child care center.</td>
  </tr>
  <tr>
    <td>Reference</td>
    <td>Pendant ce temps, des questions pratiques restent en suspens:
comment équilibrer le mandat de l'état qui garantit un accès approprié à la
marijuana agréée et interdit l'installation de commerces de vente de cannabis
dans un rayon de 30 km autour d'une école, d'un parc, d'un terrain de jeu ou
d'une crèche.</td>
  </tr>
  <tr>
    <td>RNNEnc</td>
    <td>Il y a des questions préventives qui se posent quant à l'équilibre des
droits de l'enfant dans les limites d'une entreprise de collecte de sang.</td>
  </tr>
  <tr>
    <td>grConv</td>
    <td>De façon générale, il y a des raisons de sécurité pour que les
entreprises aient accès à des milliers de centres de pêche, d'eau ou de
recherche.</td>
  </tr>
  <tr>
    <td>Moses</td>
    <td>Et il y a des problèmes mécaniques complexes qui doivent être résolues
au cours de cette période, comme la manière d'équilibrer le mandat de "l'accès
adéquat" permis de marijuana avec l'interdiction du cannabis aux entreprises de
1000 pieds d'une école de jeu ou de parc, le service de garde.</td>
  </tr>
  </tbody>
</table>

<hr>

<h2>5. Results and Analysis</h2>

### Quantitative Analysis

<p>In this paper, we are interested in the properties of the neural machine
translation models. Specifically, the translation quality with respect to the
length of source and/or target sentences and with respect to the number of words
unknown to the model in each source/target sentence.</p>

<p>First, we look at how the BLEU score, reflecting the translation performance,
changes with respect to the length of the sentences (see
Fig.~<a href="#fig:bleu_length" class="cross-ref">Fig. 4</a> (a)--(b)). Clearly, both models perform relatively
well on short sentences, but suffer significantly as the length of the
sentences increases.</p>

<p>We observe a similar trend with the number of unknown words, in
Fig.~<a href="#fig:bleu_length" class="cross-ref">Fig. 4</a> (c). As expected, the performance degrades rapidly as
the number of unknown words increases. This suggests that it will be an
important challenge to increase the size of vocabularies used by the neural
machine translation system in the future. Although we only present the result
with the RNNenc, we observed similar behavior for the grConv as well.</p>

<p>In Table~<a href="#tab:bleu" class="cross-ref">Table 1</a> (a), we present the translation performances obtained
using the two models along with the baseline phrase-based SMT system. Clearly the phrase-based SMT system still shows the superior performance over
the proposed purely neural machine translation system, but we can see that under
certain conditions (no unknown words in both source and reference sentences),
the difference diminishes quite significantly. Furthermore, if we consider only
short sentences (10--20 words per sentence), the difference further decreases
(see Table~<a href="#tab:bleu" class="cross-ref">Table 1</a> (b).</p>

<p>Furthermore, it is possible to use the neural machine translation models
together with the existing phrase-based system, which was found recently in
<sup class="ref-badge" data-ref="3" data-title="Schwenk, and Yoshua Bengio.">3</sup><sup class="ref-badge" data-ref="11" data-title="2014.">11</sup> to improve the overall translation performance
(see Table~<a href="#tab:bleu" class="cross-ref">Table 1</a> (a)).</p>

<p>This analysis suggests that that the current neural translation approach has
its weakness in handling long sentences. The most obvious explanatory
hypothesis is that the fixed-length vector representation does not have enough
capacity to encode a long sentence with complicated structure and meaning. In
order to encode a variable-length sequence, a neural network may ``sacrifice''
some of the important topics in the input sentence in order to remember others.</p>

<p>This is in stark contrast to the conventional phrase-based machine translation
system~<sup class="ref-badge" data-ref="8" data-title="2003.">8</sup>. As we can see from Fig.~<a href="#fig:moses_bleu_length" class="cross-ref">Fig. 5</a>,
the conventional system trained on the same dataset (with additional monolingual
data for the language model) tends to get a higher BLEU score on longer
sentences.</p>

<p>In fact, if we limit the lengths of both the source sentence and the reference
translation to be between 10 and 20 words and use only the sentences with no
unknown words, the BLEU scores on the test set are 27.81 and 33.08 for the
RNNenc and Moses, respectively.</p>

<p>Note that we observed a similar trend even when we used sentences of up to 50
words to train these models.</p>

### Qualitative Analysis

<p>Although BLEU score is used as a de-facto standard metric for evaluating the
performance of a machine translation system, it is not the perfect metric (see,
e.g., <sup class="ref-badge" data-ref="10" data-title="2013.">10</sup><sup class="ref-badge" data-ref="9" data-title="2011.">9</sup>). Hence, here we present some of the actual
translations generated from the two models, RNNenc and grConv.</p>

<p>In Table.~<a href="#tbl:translations" class="cross-ref">Table 2</a> (a)--(b), we show the translations of some
randomly selected sentences from the development and test sets. We chose the
ones that have no unknown words. (a) lists long sentences (longer than 30
words), and (b) short sentences (shorter than 10 words). We can see that,
despite the difference in the BLEU scores, all three models (RNNenc, grConv and
Moses) do a decent job at translating, especially, short sentences. When the
source sentences are long, however, we notice the performance degradation of the
neural machine translation models.</p>

<p>Additionally, we present here what type of structure the proposed gated
recursive convolutional network learns to represent. With a sample sentence
<em>``Obama is the President of the United States''</em>, we present the parsing
structure learned by the grConv encoder and the generated translations, in
Fig.~<a href="#fig:obama" class="cross-ref">Fig. 6</a>. The figure suggests that the grConv extracts the vector
representation of the sentence by first merging <em>``of the United States''</em>
together with <em>``is the President of''</em> and finally combining this with <em>``Obama is''</em> and <em>``.''</em>, which is well correlated with our intuition.</p>

<p>Despite the lower performance the grConv showed compared
to the RNN Encoder--Decoder,
we find this property of the grConv learning a
grammar structure automatically interesting and believe further investigation is
needed.</p>

<hr>

<h2>6. Conclusion and Discussion</h2>

<p>In this paper, we have investigated the property of a recently introduced family
of machine translation system based purely on neural networks. We focused on
evaluating an encoder--decoder approach, proposed recently in
<sup class="ref-badge" data-ref="7" data-title="2013.">7</sup><sup class="ref-badge" data-ref="3" data-title="Schwenk, and Yoshua Bengio.">3</sup><sup class="ref-badge" data-ref="11" data-title="2014.">11</sup>, on the task of
sentence-to-sentence translation. Among many possible encoder--decoder models we
specifically chose two models that differ in the choice of the encoder; (1) RNN
with gated hidden units and (2) the newly proposed gated recursive convolutional
neural network.</p>

<p>After training those two models on pairs of English and French sentences, we
analyzed their performance using BLEU scores with respect to the lengths of
sentences and the existence of unknown/rare words in sentences. Our analysis
revealed that the performance of the neural machine translation suffers
significantly from the length of sentences. However, qualitatively, we found
that the both models are able to generate correct translations very well.</p>

<p>These analyses suggest a number of future research directions in machine
translation purely based on neural networks.</p>

<p>Firstly, it is important to find a way to scale up training a neural network
both in terms of computation and memory so that much larger vocabularies for
both source and target languages can be used. Especially, when it comes to
languages with rich morphology, we may be required to come up with a radically
different approach in dealing with words.</p>

<p>Secondly, more research is needed to prevent the neural machine translation
system from underperforming with long sentences.  Lastly, we need to explore
different neural architectures, especially for the decoder. Despite the radical
difference in the architecture between RNN and grConv which were used as an
encoder, both models suffer from <em>the curse of sentence length</em>. This
suggests that it may be due to the lack of representational power in the
decoder. Further investigation and research are required.</p>

<p>In addition to the property of a general neural machine translation system, we
observed one interesting property of the proposed gated recursive convolutional
neural network (grConv). The grConv was found to mimic the grammatical structure
of an input sentence without any supervision on syntactic structure of language.
We believe this property makes it appropriate for natural language processing
applications other than machine translation.</p>

<hr>

<h2>7. Acknowledgments</h2>

<p>The authors would like to acknowledge the support of the following agencies for
research funding and computing support: NSERC, Calcul Qu\'{e}bec, Compute Canada,
the Canada Research Chairs and CIFAR.</p>

<p>\bibliographystyle{acl}
\bibliography{strings,strings-shorter,ml,aigaion,myref}</p>

<hr>

<h2>References</h2>
<ol class="references">
  <li id="ref-1"><strong>Amittai Axelrod, Xiaodong He, and Jianfeng Gao.</strong> 2011.. <em>Domain adaptation via pseudo in-domain data selection. In Proceedings of the ACL Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 355--362. Association for Computational Linguistics.</em></li>
  <li id="ref-2"><strong>Nicolas Boulanger-Lewandowski, Yoshua Bengio, and Pascal Vincent.</strong> 2013.. <em>Audio chord recognition with recurrent neural networks. In ISMIR.</em></li>
  <li id="ref-3"><strong>Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger</strong> Schwenk, and Yoshua Bengio.. <em>2014. Learning phrase representations using rnn encoder-decoder for statistical machine translation. In Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014), October. to appear.</em></li>
  <li id="ref-4"><strong>Alex Graves.</strong> 2012.. <em>Sequence transduction with recurrent neural networks. In Proceedings of the 29th International Conference on Machine Learning (ICML 2012).</em></li>
  <li id="ref-5"><strong>A. Graves.</strong> 2013.. <em>Generating sequences with recurrent neural networks. arXiv:\tt 1308.0850 [cs.NE], August.</em></li>
  <li id="ref-6"><strong>S. Hochreiter and J. Schmidhuber.</strong> 1997.. <em>Long short-term memory. Neural Computation, 9(8):1735--1780.</em></li>
  <li id="ref-7"><strong>Nal Kalchbrenner and Phil Blunsom.</strong> 2013.. <em>Two recurrent continuous translation models. In Proceedings of the ACL Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1700--1709. Association for Computational Linguistics.</em></li>
  <li id="ref-8"><strong>Philipp Koehn, Franz Josef Och, and Daniel Marcu.</strong> 2003.. <em>Statistical phrase-based translation. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, NAACL '03, pages 48--54, Stroudsburg, PA, USA. Association for Computational Linguistics.</em></li>
  <li id="ref-9"><strong>Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng.</strong> 2011.. <em>Better evaluation metrics lead to better machine translation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 375--384. Association for Computational Linguistics.</em></li>
  <li id="ref-10"><strong>Xingyi Song, Trevor Cohn, and Lucia Specia.</strong> 2013.. <em>BLEU deconstructed: Designing a better MT evaluation metric. In Proceedings of the 14th International Conference on Intelligent Text Processing and Computational Linguistics (CICLING), March.</em></li>
  <li id="ref-11"><strong>Ilya Sutskever, Oriol Vinyals, and Quoc Le.</strong> 2014.. <em>Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems (NIPS 2014), December.</em></li>
  <li id="ref-12"><strong>Matthew D. Zeiler.</strong> 2012.. <em>ADADELTA: an adaptive learning rate method. Technical report, arXiv 1212.5701. \endthebibliography</em></li>
</ol>
