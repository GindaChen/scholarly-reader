<h1>Sequence to Sequence Learningwith Neural Networks</h1>

<p class="authors"></p>

<h2>Abstract</h2>
<p>Deep Neural Networks (DNNs) are powerful models that have achieved
excellent performance on difficult learning tasks. Although DNNs work
well whenever large labeled training sets are available, 
they cannot be used to map sequences to sequences.  In this paper, we
present a general end-to-end approach to sequence learning that makes
minimal assumptions on the sequence structure. Our method uses a
multilayered Long Short-Term Memory (LSTM) to map the input sequence
to a vector of a fixed dimensionality, and then another deep LSTM to
decode the target sequence from the vector.  Our main result is that
on an English to French translation task from the WMT'14 dataset, the
translations produced by the LSTM achieve a BLEU score of 34.8 on the
entire test set, where the LSTM's BLEU score was penalized on
out-of-vocabulary words. Additionally, the LSTM did not have
difficulty on long sentences. For comparison, a phrase-based
SMT system achieves a BLEU score of 33.3 on the same dataset.  When we
used the LSTM to rerank the 1000 hypotheses produced by the
aforementioned SMT system, its BLEU score increases to 36.5, which
is close to the previous best result on this task.  The LSTM also learned sensible
phrase and sentence representations that are sensitive to word order
and are relatively invariant to the active and the passive voice.
Finally, we found that reversing the order of the words in all source
sentences (but not target sentences) improved the LSTM's performance
markedly, because doing so introduced many short term dependencies
between the source and the target sentence which made the optimization
problem easier.</p>

<hr>

<h2>1. Introduction</h2>

<p>Deep Neural Networks (DNNs) are extremely powerful machine learning
models that achieve excellent performance on difficult problems such
as speech recognition [hinton12][dahl12b] and visual object
recognition [kriz12][ciresan12][lecun98][le12].  DNNs are
powerful because they can perform arbitrary parallel computation for a
modest number of steps.  A surprising example of the power of DNNs is
their ability to sort <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span></span> \!\!\!\!\quad <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span></span>-bit numbers using only 2
hidden layers of quadratic size [razborov]. So, while neural
networks are related to conventional statistical models, they learn an
intricate computation.  Furthermore, large DNNs can be trained with
supervised backpropagation whenever the labeled training set has
enough information to specify the network's parameters.  Thus, if
there exists a parameter setting of a large DNN that achieves good
results (for example, because humans can solve the task very rapidly),
supervised backpropagation will find these parameters and solve the
problem.</p>

<p>Despite their flexibility and power, DNNs can only be applied to
problems whose inputs and targets can be sensibly encoded with vectors
of fixed dimensionality.  It is a significant limitation, since many
important problems are best expressed with sequences whose lengths are
not known a-priori.  For example, speech recognition and machine
translation are sequential problems.  Likewise, question answering can
also be seen as mapping a sequence of words representing the question
to a sequence of words representing the answer.  It is therefore clear
that a domain-independent method that learns to map sequences to
sequences would be useful.</p>

<p>Sequences pose a challenge for DNNs because they require that the
dimensionality of the inputs and outputs is known and fixed.</p>

<p>In this paper, we show that a straightforward application of the Long
Short-Term Memory (LSTM) architecture [hochreiter97] can solve
general sequence to sequence problems.  The idea is to use one LSTM to
read the input sequence, one timestep at a time, to obtain large
fixed-dimensional vector representation, and then to use another LSTM
to extract the output sequence from that vector
(fig.~<a href="#fig:translation-model2" class="cross-ref">Fig. 1</a>).  The second LSTM is essentially a
recurrent neural network language model
[rumelhart1986learning][mikolov2010recurrent][sundermeyer12] except
that it is conditioned on the input sequence.  The LSTM's ability to
successfully learn on data with long range temporal dependencies makes
it a natural choice for this application due to the considerable time
lag between the inputs and their corresponding outputs
(fig.~<a href="#fig:translation-model2" class="cross-ref">Fig. 1</a>).</p>

<p>There have been a number of related attempts to address the general
sequence to sequence learning problem with neural networks.   Our
approach is closely related to Kalchbrenner and Blunsom [kal13] who were
the first to map the entire input sentence to vector, and is related 
to Cho et al.~[cho14] although the latter was used only for rescoring hypotheses
produced by a phrase-based system.  Graves
[graves13c] introduced a novel differentiable attention mechanism
that allows neural networks to focus on different parts
of their input, and an elegant variant of this idea was successfully
applied to machine translation by Bahdanau et al.~[bog14].  The Connectionist
Sequence Classification is another popular technique for mapping sequences
to sequences with neural networks, but it assumes a monotonic alignment
between the inputs and the outputs [graves1].</p>

<p>The main result of this work is the following.  On the WMT'14 English
to French translation task, we obtained a BLEU score of {\bf 34.81} by
directly extracting translations from an ensemble of 5 deep LSTMs
(with 384M parameters and 8,000 dimensional state each) using a simple left-to-right beam-search
decoder.  This is by far the best result achieved by direct
translation with large neural networks.  For comparison, the BLEU
score of an SMT baseline on
this dataset is 33.30 [wmt14_en_fr].  The
34.81 BLEU score was achieved by an LSTM with a vocabulary of 80k
words, so the score was penalized whenever the reference translation
contained a word not covered by these 80k.  This result shows that a
relatively unoptimized small-vocabulary neural network architecture which has much room
for improvement outperforms a phrase-based SMT system.</p>

<p>Finally, we used the LSTM to rescore the publicly available 1000-best
lists of the SMT baseline on the same task [wmt14_en_fr].  By
doing so, we obtained a BLEU score of 36.5, which improves the
baseline by 3.2 BLEU points and is close to the previous best published
result on this task (which is 37.0 [durrani-EtAl:2014:W14-33]).</p>

<p>Surprisingly, the LSTM did not suffer on very long sentences, despite
the recent experience of other researchers with related architectures
[curse].  We were able to do well on long sentences because we
reversed the order of words in the source sentence but not the target
sentences in the training and test set. By doing so, we introduced
many short term dependencies that made the optimization problem much
simpler (see sec.~<a href="#sec:model" class="cross-ref">Section 1</a> and <a href="#sec:rev_rev" class="cross-ref">Section 3</a>).  As a result, SGD could learn
LSTMs that had no trouble with long sentences.  The simple trick of
reversing the words in the source sentence is one of the key technical
contributions of this work.
 
A useful property of the LSTM is that it learns to map an input
sentence of variable length into a fixed-dimensional vector
representation.  Given that translations tend to be paraphrases of the
source sentences, the translation objective encourages the LSTM to
find sentence representations that capture their meaning, as sentences
with similar meanings are close to each other while different
sentences meanings will be far. A qualitative evaluation supports
this claim, showing that our model is aware of word order and is
fairly invariant to the active and passive voice.</p>

<hr>

<h2>2. The model</h2>

<span id="sec:model" class="label-anchor"></span>

<p>The Recurrent Neural Network (RNN) [werbos][rumelhart1986learning]
is a natural generalization of feedforward neural networks to
sequences.  Given a sequence of inputs <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>x</mi><mi>T</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(x_1,\ldots,x_T)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="var" data-var="x" data-desc="Input tensor" style="--var-color: #98c379"><span class="mord mathnormal">x</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="var" data-var="x" data-desc="Input tensor" style="--var-color: #98c379"><span class="mord mathnormal">x</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span>, a
standard RNN computes a sequence of outputs <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>y</mi><mi>T</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(y_1,\ldots,y_T)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span> by
iterating the following equation: 
\begin{eqnarray*}
h_t &=& \mathrm{sigm}\left(W^{\mathrm{hx}} x_t + W^{\mathrm{hh}} h_{t-1}\right) \\
y_t &=& W^{\mathrm{yh}}h_t
\end{eqnarray*}
The RNN can easily map sequences to sequences whenever the alignment
between the inputs the outputs is known ahead of time. However, it is
not clear how to apply an RNN to problems whose input and the output
sequences have different lengths with complicated and non-monotonic
relationships.</p>

<p>The simplest strategy for general sequence learning is to map the input
sequence to a fixed-sized vector using one RNN, and then to map the
vector to the target sequence with another RNN (this approach has also been
taken by Cho et al.~[cho14]).  While it could work
in principle since the RNN is provided with all the relevant
information, it would be difficult to train the RNNs due to the
resulting long term dependencies
(figure <a href="#fig:translation-model2" class="cross-ref">Fig. 1</a>)
[hochreiter_long_term][bengio_long_term][hochreiter97][Hochreiter01gradientflow]. However, the Long
Short-Term Memory (LSTM) [hochreiter97] is known to learn
problems with long range temporal dependencies, so an LSTM may succeed
in this setting.</p>

<p>The goal of the LSTM is to estimate the conditional probability
<span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy="false">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>y</mi><msup><mi>T</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>x</mi><mi>T</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p(y_1,\ldots,y_{T&#x27;} | x_1,\ldots,x_T)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6828em;"><span style="top:-2.786em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord"><span class="var" data-var="x" data-desc="Input tensor" style="--var-color: #98c379"><span class="mord mathnormal">x</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="var" data-var="x" data-desc="Input tensor" style="--var-color: #98c379"><span class="mord mathnormal">x</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span> where <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>x</mi><mi>T</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(x_1,\ldots,x_T)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="var" data-var="x" data-desc="Input tensor" style="--var-color: #98c379"><span class="mord mathnormal">x</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="var" data-var="x" data-desc="Input tensor" style="--var-color: #98c379"><span class="mord mathnormal">x</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span> is an
input sequence and <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>y</mi><msup><mi>T</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></msub></mrow><annotation encoding="application/x-tex">y_1,\ldots,y_{T&#x27;}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6828em;"><span style="top:-2.786em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span> is its corresponding output
sequence whose length <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>T</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></mrow><annotation encoding="application/x-tex">T&#x27;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7519em;"></span><span class="mord"><span class="var" data-var="T" data-desc="Transpose" style="--var-color: #f0a050"><span class="mord mathnormal" style="margin-right:0.13889em;">T</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span> may differ from <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi></mrow><annotation encoding="application/x-tex">T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="var" data-var="T" data-desc="Transpose" style="--var-color: #f0a050"><span class="mord mathnormal" style="margin-right:0.13889em;">T</span></span></span></span></span></span>. The LSTM computes this
conditional probability by first obtaining the fixed-dimensional
representation <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span></span></span></span></span> of the input sequence <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>x</mi><mi>T</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(x_1,\ldots,x_T)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="var" data-var="x" data-desc="Input tensor" style="--var-color: #98c379"><span class="mord mathnormal">x</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="var" data-var="x" data-desc="Input tensor" style="--var-color: #98c379"><span class="mord mathnormal">x</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span> given by
the last hidden state of the LSTM, and then computing the probability
of <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>y</mi><msup><mi>T</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></msub></mrow><annotation encoding="application/x-tex">y_1,\ldots,y_{T&#x27;}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6828em;"><span style="top:-2.786em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span> with a standard LSTM-LM formulation whose
initial hidden state is set to the representation <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span></span></span></span></span> of
<span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>x</mi><mi>T</mi></msub></mrow><annotation encoding="application/x-tex">x_1,\ldots,x_T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="var" data-var="x" data-desc="Input tensor" style="--var-color: #98c379"><span class="mord mathnormal">x</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="var" data-var="x" data-desc="Input tensor" style="--var-color: #98c379"><span class="mord mathnormal">x</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span>:</p>

<div class="math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>p</mi><mo stretchy="false">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>y</mi><msup><mi>T</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>x</mi><mi>T</mi></msub><mo stretchy="false">)</mo><mo>=</mo><munderover><mo>∏</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><msup><mi>T</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></munderover><mi>p</mi><mo stretchy="false">(</mo><msub><mi>y</mi><mi>t</mi></msub><mi mathvariant="normal">∣</mi><mi>v</mi><mo separator="true">,</mo><msub><mi>y</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>y</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p(y_1,\ldots,y_{T&#x27;} | x_1,\ldots,x_T) = \prod_{t=1}^{T&#x27;} p(y_t | v, y_1, \ldots, y_{t-1})
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6828em;"><span style="top:-2.786em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord"><span class="var" data-var="x" data-desc="Input tensor" style="--var-color: #98c379"><span class="mord mathnormal">x</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="var" data-var="x" data-desc="Input tensor" style="--var-color: #98c379"><span class="mord mathnormal">x</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:3.1966em;vertical-align:-1.2671em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.9295em;"><span style="top:-1.8829em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∏</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8278em;"><span style="top:-2.931em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2671em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></div>

<p>In this equation, each <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy="false">(</mo><msub><mi>y</mi><mi>t</mi></msub><mi mathvariant="normal">∣</mi><mi>v</mi><mo separator="true">,</mo><msub><mi>y</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>y</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p(y_t | v, y_1, \ldots, y_{t-1})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span> distribution
is represented with a softmax over all the words in the
vocabulary.  We use the LSTM formulation from Graves [graves13c].
Note that we require that each sentence ends with a special
end-of-sentence symbol ``<span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>&lt;</mo></mrow><annotation encoding="application/x-tex">&lt;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em;"></span><span class="mrel">&lt;</span></span></span></span></span>EOS<span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>&gt;</mo></mrow><annotation encoding="application/x-tex">&gt;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em;"></span><span class="mrel">&gt;</span></span></span></span></span>'', which enables the model to define a
distribution over sequences of all possible lengths. The overall
scheme is outlined in figure <a href="#fig:translation-model2" class="cross-ref">Fig. 1</a>, where the
shown LSTM computes the representation of ``A'', ``B'', ``C'', ``<span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>&lt;</mo></mrow><annotation encoding="application/x-tex">&lt;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em;"></span><span class="mrel">&lt;</span></span></span></span></span>EOS<span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>&gt;</mo></mrow><annotation encoding="application/x-tex">&gt;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em;"></span><span class="mrel">&gt;</span></span></span></span></span>''
and then uses this representation to compute the probability of ``W'',
``X'', ``Y'', ``Z'', ``<span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>&lt;</mo></mrow><annotation encoding="application/x-tex">&lt;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em;"></span><span class="mrel">&lt;</span></span></span></span></span>EOS<span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>&gt;</mo></mrow><annotation encoding="application/x-tex">&gt;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em;"></span><span class="mrel">&gt;</span></span></span></span></span>''.</p>

<p>Our actual models differ from the above description in three important
ways.  First, we used two different LSTMs: one for the input sequence
and another for the output sequence, because doing so increases the
number model parameters at negligible computational cost and makes it
natural to train the LSTM on multiple language pairs simultaneously
[kal13].  Second, we found that deep LSTMs significantly
outperformed shallow LSTMs, so we chose an LSTM with four layers.
Third, we found it extremely valuable to reverse the order of the
words of the input sentence. So for example, instead of mapping the
sentence <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mo separator="true">,</mo><mi>b</mi><mo separator="true">,</mo><mi>c</mi></mrow><annotation encoding="application/x-tex">a,b,c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="var" data-var="b" data-desc="Bias vector" style="--var-color: #ffd700"><span class="mord mathnormal">b</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">c</span></span></span></span></span> to the sentence <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo separator="true">,</mo><mi>β</mi><mo separator="true">,</mo><mi>γ</mi></mrow><annotation encoding="application/x-tex">\alpha, \beta, \gamma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span></span></span></span></span>, the LSTM is
asked to map <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi><mo separator="true">,</mo><mi>b</mi><mo separator="true">,</mo><mi>a</mi></mrow><annotation encoding="application/x-tex">c,b,a</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">c</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="var" data-var="b" data-desc="Bias vector" style="--var-color: #ffd700"><span class="mord mathnormal">b</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">a</span></span></span></span></span> to <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo separator="true">,</mo><mi>β</mi><mo separator="true">,</mo><mi>γ</mi></mrow><annotation encoding="application/x-tex">\alpha, \beta, \gamma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span></span></span></span></span>, where <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo separator="true">,</mo><mi>β</mi><mo separator="true">,</mo><mi>γ</mi></mrow><annotation encoding="application/x-tex">\alpha, \beta,
\gamma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span></span></span></span></span> is the translation of <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mo separator="true">,</mo><mi>b</mi><mo separator="true">,</mo><mi>c</mi></mrow><annotation encoding="application/x-tex">a,b,c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="var" data-var="b" data-desc="Bias vector" style="--var-color: #ffd700"><span class="mord mathnormal">b</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">c</span></span></span></span></span>.  This way, <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi></mrow><annotation encoding="application/x-tex">a</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">a</span></span></span></span></span> is in close
proximity to <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span></span></span></span>, <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="var" data-var="b" data-desc="Bias vector" style="--var-color: #ffd700"><span class="mord mathnormal">b</span></span></span></span></span></span> is fairly close to <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05278em;">β</span></span></span></span></span>, and so on, a
fact that makes it easy for SGD to ``establish communication'' between
the input and the output.  We found this simple data transformation to
greatly improve the performance of the LSTM.</p>

<hr>

<h2>3. Experiments</h2>

<span id="sec:experiments" class="label-anchor"></span>
 
We applied our method to the WMT'14 English to French MT task in two
ways.  We used it to directly translate the input sentence without
using a reference SMT system and we it to rescore the n-best lists of
an SMT baseline.  We report the accuracy of these translation methods,
present sample translations, and visualize the resulting sentence
representation.

### Dataset details

<p>We used the WMT'14 English to French dataset.  We trained our models
on a subset of 12M sentences consisting of 348M French words and 304M
English words, which is a clean ``selected'' subset from
[wmt14_en_fr]. We chose this translation task and this specific
training set subset because of the public availability of a tokenized training and
test set together with 1000-best lists from the baseline SMT 
[wmt14_en_fr].</p>

<p>As typical neural language models rely on a vector representation for
each word, we used a fixed vocabulary for both languages.  We used
160,000 of the most frequent words for the source language and 80,000
of the most frequent words for the target language.  Every
out-of-vocabulary word was replaced with a special ``UNK'' token.</p>

### Decoding and Rescoring

<p>The core of our experiments involved training a large deep LSTM on
many sentence pairs. We trained it by maximizing the log
probability of a correct translation <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi></mrow><annotation encoding="application/x-tex">T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="var" data-var="T" data-desc="Transpose" style="--var-color: #f0a050"><span class="mord mathnormal" style="margin-right:0.13889em;">T</span></span></span></span></span></span> given the source sentence
<span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi></mrow><annotation encoding="application/x-tex">S</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span></span></span></span></span>, so the training objective is</p>

<div class="math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mn>1</mn><mi mathvariant="normal">/</mi><mi mathvariant="normal">∣</mi><mi mathvariant="script">S</mi><mi mathvariant="normal">∣</mi><munder><mo>∑</mo><mrow><mo stretchy="false">(</mo><mi>T</mi><mo separator="true">,</mo><mi>S</mi><mo stretchy="false">)</mo><mo>∈</mo><mi mathvariant="script">S</mi></mrow></munder><mi>log</mi><mo>⁡</mo><mi>p</mi><mo stretchy="false">(</mo><mi>T</mi><mi mathvariant="normal">∣</mi><mi>S</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">1/|\mathcal S|\sum_{(T,S)\in\mathcal S}\log p(T|S)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.566em;vertical-align:-1.516em;"></span><span class="mord">1/∣</span><span class="mord mathcal" style="margin-right:0.075em;">S</span><span class="mord">∣</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.05em;"><span style="top:-1.809em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.05764em;">S</span><span class="mclose mtight">)</span><span class="mrel mtight">∈</span><span class="mord mathcal mtight" style="margin-right:0.075em;">S</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.516em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="var" data-var="T" data-desc="Transpose" style="--var-color: #f0a050"><span class="mord mathnormal" style="margin-right:0.13889em;">T</span></span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mclose">)</span></span></span></span></span></div>
 where <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">S</mi></mrow><annotation encoding="application/x-tex">\mathcal
S</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathcal" style="margin-right:0.075em;">S</span></span></span></span></span> is the training set.  Once training is complete, we produce
translations by finding the most likely translation according to
the LSTM:

<div class="math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mover accent="true"><mi>T</mi><mo>^</mo></mover><mo>=</mo><mi>arg</mi><mo>⁡</mo><munder><mrow><mi>max</mi><mo>⁡</mo></mrow><mi>T</mi></munder><mi>p</mi><mo stretchy="false">(</mo><mi>T</mi><mi mathvariant="normal">∣</mi><mi>S</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">
\hat{T} = \arg\max_T p(T|S)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9468em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9468em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="var" data-var="T" data-desc="Transpose" style="--var-color: #f0a050"><span class="mord mathnormal" style="margin-right:0.13889em;">T</span></span></span><span style="top:-3.2523em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1667em;"><span class="mord">^</span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.4943em;vertical-align:-0.7443em;"></span><span class="mop">ar<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.4306em;"><span style="top:-2.3557em;margin-left:0em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span><span class="mop">max</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.7443em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="var" data-var="T" data-desc="Transpose" style="--var-color: #f0a050"><span class="mord mathnormal" style="margin-right:0.13889em;">T</span></span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mclose">)</span></span></span></span></span></div>

<p>We search for the most likely translation using a simple left-to-right
beam search decoder which maintains a small number <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi></mrow><annotation encoding="application/x-tex">B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span></span></span></span></span> of partial
hypotheses, where a partial hypothesis is a prefix of some
translation.  At each timestep we extend each partial hypothesis in
the beam with every possible word in the vocabulary. This greatly
increases the number of the hypotheses so we discard all but the <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi></mrow><annotation encoding="application/x-tex">B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span></span></span></span></span>
most likely hypotheses according to the model's log probability.  As soon
as the ``<span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>&lt;</mo></mrow><annotation encoding="application/x-tex">&lt;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em;"></span><span class="mrel">&lt;</span></span></span></span></span>EOS<span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>&gt;</mo></mrow><annotation encoding="application/x-tex">&gt;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em;"></span><span class="mrel">&gt;</span></span></span></span></span>'' symbol is appended to a hypothesis, it is removed from
the beam and is added to the set of complete hypotheses.  While this
decoder is approximate, it is simple to implement.  Interestingly, our
system performs well even with a beam size of 1, and a beam of
size 2 provides most of the benefits of beam search (Table
<a href="#tab:blue_fr" class="cross-ref">Table 1</a>).</p>

<p>We also used the LSTM to rescore the 1000-best lists produced by the
baseline system [wmt14_en_fr].  To rescore an n-best list, we
computed the log probability of every hypothesis with our LSTM and
took an even average with their score and the LSTM's score.</p>

### Reversing the Source Sentences
<span id="sec:rev_rev" class="label-anchor"></span>

<p>While the LSTM is capable of solving problems with long term
dependencies, we discovered that the LSTM learns much better when the
source sentences are reversed (the target sentences are not reversed).  By
doing so, the LSTM's test perplexity dropped from 5.8 to 4.7, and the 
test BLEU scores of its decoded translations increased from 25.9 to 30.6.</p>

<p>While we do not have a complete explanation to this phenomenon, we
believe that it is caused by the introduction of many short term
dependencies to the dataset.  Normally, when we concatenate a source
sentence with a target sentence, each word in the source sentence is
far from its corresponding word in the target sentence. As a result,
the problem has a large ``minimal time lag'' [minimal_time_lag].  By reversing the
words in the source sentence, the average distance between
corresponding words in the source and target language is unchanged.
However, the first few words in the source language are now very close to
the first few words in the target language, so the problem's minimal
time lag is greatly reduced. Thus, backpropagation has an easier time
``establishing communication'' between the source sentence and the
target sentence, which in turn results in substantially improved overall
 performance.</p>

<p>Initially, we believed that reversing the input sentences would 
only lead to more confident predictions in the early parts of the target
sentence and to less confident predictions in the later parts.
However, LSTMs trained on reversed source sentences did much better on
long sentences than LSTMs trained on the raw source sentences (see
sec.~<a href="#sec:long_sentences" class="cross-ref">Section 4</a>), which suggests that reversing the
input sentences results in LSTMs with better memory utilization.</p>

### Training details

<p>We found that the LSTM models are fairly easy to train.  We used deep
LSTMs with 4 layers, with 1000 cells at each layer and 1000
dimensional word embeddings, with an input vocabulary of 160,000
and an output vocabulary of 80,000.  Thus the deep LSTM uses 8000 real 
numbers to represent a sentence. We found deep LSTMs to
significantly outperform shallow LSTMs, where each additional layer
reduced perplexity by nearly 10\%, possibly due to their much larger
hidden state.  We used a naive softmax over 80,000 words at each
output.  The resulting LSTM has 384M parameters of which 64M are pure
recurrent connections (32M for the ``encoder'' LSTM and 32M for the
``decoder'' LSTM). The complete training details are given below:
- We initialized all of the LSTM's parameters with the uniform distribution between
  -0.08 and 0.08
- We used stochastic gradient descent without momentum,
  with a fixed learning rate of 0.7.  After 5 epochs, we begun
  halving the learning rate every half epoch.  We trained our models for a
  total of 7.5 epochs.
- We used batches of 128 sequences for the gradient and divided it
  the size of the batch (namely, 128).
- Although LSTMs tend to not suffer from the vanishing gradient
  problem, they can have exploding gradients.  Thus we enforced a hard
  constraint on the norm of the gradient
  [graves13c][razvan] by scaling it when its norm exceeded
  a threshold. For each training batch, we compute <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mo>=</mo><msub><mrow><mo fence="true">∥</mo><mi>g</mi><mo fence="true">∥</mo></mrow><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">s =
  \left\|g\right\|_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">s</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0497em;vertical-align:-0.2997em;"></span><span class="minner"><span class="minner"><span class="mopen delimcenter" style="top:0em;">∥</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mclose delimcenter" style="top:0em;">∥</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.4003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2997em;"><span></span></span></span></span></span></span></span></span></span></span>, where <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi></mrow><annotation encoding="application/x-tex">g</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span></span></span></span></span> is the gradient divided by 128. If <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mo>&gt;</mo><mn>5</mn></mrow><annotation encoding="application/x-tex">s &gt; 5</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em;"></span><span class="mord mathnormal">s</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">5</span></span></span></span></span>, we set
  <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mo>=</mo><mfrac><mrow><mn>5</mn><mi>g</mi></mrow><mi>s</mi></mfrac></mrow><annotation encoding="application/x-tex">g = \frac{5g}{s}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.2422em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8972em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">s</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.4461em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">5</span><span class="mord mathnormal mtight" style="margin-right:0.03588em;">g</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span>.
- Different sentences have different lengths.  Most sentences are
  short (e.g., length 20-30) but some sentences are long (e.g., length
  <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>&gt;</mo></mrow><annotation encoding="application/x-tex">&gt;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em;"></span><span class="mrel">&gt;</span></span></span></span></span> 100), so a minibatch of 128 randomly chosen training sentences
  will have many short sentences and few long sentences, and as a
  result, much of the computation in the minibatch is wasted.  To
  address this problem, we made sure that all sentences in a
  minibatch are roughly of the same length, yielding a 2x speedup.</p>

### Parallelization

<p>A C++ implementation of deep LSTM with the configuration from the
previous section on a single GPU processes a speed of approximately
1,700 words per second.  This was too slow for our purposes, so we 
parallelized our model using an 8-GPU machine.  Each layer of the LSTM
was executed on a different GPU and communicated its activations
to the next GPU / layer as soon as they were computed.  Our models
have 4 layers of LSTMs, each of which resides on a separate GPU.  The remaining
4 GPUs were used to parallelize the softmax, so each GPU was
responsible for multiplying by a <span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1000</mn><mo>×</mo><mn>20000</mn></mrow><annotation encoding="application/x-tex">1000\times 20000</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">1000</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">20000</span></span></span></span></span> matrix.  The
resulting implementation achieved a speed of 6,300 (both English and
French) words per second with a minibatch size of 128. 
Training took about a ten days with this implementation.</p>

### Experimental Results

<p>We used the cased BLEU score [bleu] to evaluate the quality of our
translations. We computed our BLEU scores using \texttt{multi-bleu.pl} 
on the *tokenized* predictions and ground truth.
This way of evaluating the BELU score is consistent with [cho14] and [bog14], and reproduces
the 33.3 score of [wmt14_en_fr].
However, if we evaluate the best WMT'14 system [durrani-EtAl:2014:W14-33]
(whose predictions can be downloaded from [statmt.org\matrix](statmt.org\matrix)) in this manner, we get   
37.0, which is greater than the 35.8 reported by [statmt.org\matrix](statmt.org\matrix).</p>

<p>The results are presented in tables <a href="#tab:blue_fr" class="cross-ref">Table 1</a> and
<a href="#tab:blue_fr_rescore" class="cross-ref">Table 2</a>.  Our best results are obtained with an
ensemble of LSTMs that differ in their random initializations and
in the random order of minibatches.  While the decoded translations of the
LSTM ensemble do not outperform the best WMT'14 system, it is the first time
that a pure neural translation system outperforms a 
phrase-based SMT baseline on a large scale MT task by a sizeable margin,
despite its inability to handle out-of-vocabulary words.  The LSTM
is within 0.5 BLEU points of the best WMT'14 result if it is used to rescore the 1000-best
list of the baseline system.</p>

<table class="article-table">
  <caption>The performance of the LSTM on WMT'14 English to French test
  set (ntst14).  Note that an ensemble of 5 LSTMs with a beam of size
  2 is cheaper than of a single LSTM with a beam of size 12.</caption>
  <thead><tr>
    <th>{\bf Method}</th>
    <th>{\bf test BLEU score (ntst14) }</th>
  </tr></thead>
  <tbody>
  <tr>
    <td>Bahdanau et al. [bog14]</td>
    <td>28.45</td>
  </tr>
  <tr>
    <td>Baseline System  [wmt14_en_fr]</td>
    <td>33.30</td>
  </tr>
  <tr>
    <td>Single forward LSTM, beam size 12</td>
    <td>26.17</td>
  </tr>
  <tr>
    <td>Single reversed LSTM, beam size 12</td>
    <td>30.59</td>
  </tr>
  <tr>
    <td>Ensemble of 5 reversed LSTMs, beam size 1</td>
    <td>33.00</td>
  </tr>
  <tr>
    <td>Ensemble of 2 reversed LSTMs, beam size 12</td>
    <td>33.27</td>
  </tr>
  <tr>
    <td>Ensemble of 5 reversed LSTMs, beam size 2</td>
    <td>34.50</td>
  </tr>
  <tr>
    <td>Ensemble of 5 reversed LSTMs, beam size 12</td>
    <td>{\bf 34.81}</td>
  </tr>
  </tbody>
</table>

<table class="article-table">
  <caption>Methods that use neural networks together with an SMT system
  on the WMT'14 English to French test set (ntst14).</caption>
  <thead><tr>
    <th>{\bf Method}</th>
    <th>{\bf test BLEU score (ntst14) }</th>
  </tr></thead>
  <tbody>
  <tr>
    <td>Baseline System  [wmt14_en_fr]</td>
    <td>33.30</td>
  </tr>
  <tr>
    <td>Cho et al. [cho14]</td>
    <td>34.54</td>
  </tr>
  <tr>
    <td>Best WMT'14 result [durrani-EtAl:2014:W14-33]</td>
    <td>{\bf 37.0}</td>
  </tr>
  <tr>
    <td>Rescoring the baseline 1000-best with a single forward LSTM</td>
    <td>35.61</td>
  </tr>
  <tr>
    <td>Rescoring the baseline 1000-best with a single reversed  LSTM</td>
    <td>35.85</td>
  </tr>
  <tr>
    <td>Rescoring the baseline 1000-best with an ensemble of 5 reversed LSTMs</td>
    <td>{\bf 36.5}</td>
  </tr>
  <tr>
    <td>Oracle Rescoring of the Baseline 1000-best lists</td>
    <td><span class="math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>∼</mo></mrow><annotation encoding="application/x-tex">\sim</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.3669em;"></span><span class="mrel">∼</span></span></span></span></span>45</td>
  </tr>
  </tbody>
</table>

### Performance on long sentences
<span id="sec:long_sentences" class="label-anchor"></span>

<p>We were surprised to discover that the LSTM did well on long
sentences, which is shown quantitatively in figure <a href="#fig:oriol" class="cross-ref">Fig. 3</a>.
Table <a href="#tab:examples" class="cross-ref">Table 4</a> presents several examples of long sentences and
their translations.</p>

<table class="article-table">
  <caption>A few examples of long translations produced by the LSTM
  alongside the ground truth translations.  The reader can verify that
  the translations are sensible using Google translate.</caption>
  <thead><tr>
    <th>{\bf  Type}</th>
    <th>{\bf Sentence}</th>
  </tr></thead>
  <tbody>
  <tr>
    <td>{\bf Our model}</td>
    <td>Ulrich UNK , membre du conseil d' administration du constructeur automobile Audi ,</td>
  </tr>
  <tr>
    <td></td>
    <td>affirme qu' il s' agit d' une pratique courante depuis des ann\'{e}es pour que les t\'{e}l\'{e}phones</td>
  </tr>
  <tr>
    <td></td>
    <td>portables  puissent  \^{e}tre collect\'{e}s avant les r\'{e}unions du conseil d' administration afin qu' ils</td>
  </tr>
  <tr>
    <td></td>
    <td>ne soient pas  utilis\'{e}s comme appareils d' \'{e}coute \`{a} distance .</td>
  </tr>
  <tr>
    <td>{\bf  Truth}</td>
    <td>Ulrich Hackenberg , membre du conseil d' administration du constructeur automobile Audi ,</td>
  </tr>
  <tr>
    <td></td>
    <td>d\'{e}clare que la collecte des t\'{e}l\'{e}phones portables avant les r\'{e}unions du conseil , afin qu' ils</td>
  </tr>
  <tr>
    <td></td>
    <td>ne puissent pas \^{e}tre utilis\'{e}s comme appareils d' \'{e}coute \`{a} distance , est une pratique courante</td>
  </tr>
  <tr>
    <td></td>
    <td>depuis des ann\'{e}es .</td>
  </tr>
  <tr>
    <td>{\bf Our model}</td>
    <td>`` Les t\'{e}l\'{e}phones cellulaires , qui sont vraiment une question , non seulement parce qu' ils</td>
  </tr>
  <tr>
    <td></td>
    <td>pourraient potentiellement causer des interf\'{e}rences avec les appareils de navigation , mais</td>
  </tr>
  <tr>
    <td></td>
    <td>nous savons , selon la FCC , qu' ils pourraient interf\'{e}rer avec les tours de t\'{e}l\'{e}phone cellulaire</td>
  </tr>
  <tr>
    <td></td>
    <td>lorsqu' ils sont dans l' air '' , dit UNK .</td>
  </tr>
  <tr>
    <td>{\bf Truth}</td>
    <td>`` Les t\'{e}l\'{e}phones portables sont v\'{e}ritablement un probl\`{e}me , non seulement parce qu' ils</td>
  </tr>
  <tr>
    <td></td>
    <td>pourraient \'{e}ventuellement cr\'{e}er des interf\'{e}rences avec les instruments de navigation , mais</td>
  </tr>
  <tr>
    <td></td>
    <td>parce que nous savons , d' apr\`{e}s la FCC , qu' ils pourraient perturber les antennes-relais de</td>
  </tr>
  <tr>
    <td></td>
    <td>t\'{e}l\'{e}phonie mobile s' ils sont utilis\'{e}s \`{a} bord '' , a d\'{e}clar\'{e} Rosenker .</td>
  </tr>
  <tr>
    <td>{\bf Our model}</td>
    <td>Avec la cr\'{e}mation , il y a un `` sentiment de violence contre le corps d' un \^{e}tre cher '' ,</td>
  </tr>
  <tr>
    <td></td>
    <td>qui sera `` r\'{e}duit \`{a} une pile de cendres '' en tr\`{e}s peu de temps au lieu d' un processus de</td>
  </tr>
  <tr>
    <td></td>
    <td>d\'{e}composition ``  qui accompagnera les \'{e}tapes du deuil '' .</td>
  </tr>
  <tr>
    <td>{\bf Truth}</td>
    <td>Il y a , avec la cr\'{e}mation , `` une violence faite au corps aim\'{e} '' ,</td>
  </tr>
  <tr>
    <td></td>
    <td>qui va \^{e}tre `` r\'{e}duit \`{a} un tas de cendres '' en tr\`{e}s peu de temps , et non apr\`{e}s un processus de</td>
  </tr>
  <tr>
    <td></td>
    <td>d\'{e}composition , qui `` accompagnerait les phases du deuil '' .</td>
  </tr>
  </tbody>
</table>

### Model Analysis

<p>One of the attractive features of our model is its ability to turn a
sequence of words into a vector of fixed dimensionality.
Figure~<a href="#fig:embedding" class="cross-ref">Fig. 2</a> visualizes some of the learned
representations.  The figure clearly shows that the representations
are sensitive to the order of words, while being fairly insensitive to
the replacement of an active voice with a passive voice.  The
two-dimensional projections are obtained using PCA.</p>

<hr>

<h2>4. Related work</h2>

<span id="sec:rel_work" class="label-anchor"></span>

<p>There is a large body of work on applications of neural networks to
machine translation. So far, the simplest and most effective way of
applying an RNN-Language Model (RNNLM) [mikolov2010recurrent] or
a Feedforward Neural Network Language Model (NNLM) [bengio] to an
MT task is by rescoring the n-best lists of a strong MT baseline
[mikolov2012], which reliably improves translation quality.</p>

<p>More recently, researchers have begun to look into ways of including
information about the source language into the NNLM.  Examples of this
work include Auli et al.~[auli13], who combine an NNLM with a
topic model of the input sentence, which improves rescoring
performance.  Devlin et al.~[devlin14] followed a similar
approach, but they incorporated their NNLM into the decoder of an MT
system and used the decoder's alignment information to provide the
NNLM with the most useful words in the input sentence.  Their approach
was highly successful and it achieved large improvements over their
baseline.</p>

<p>Our work is closely related to Kalchbrenner and Blunsom [kal13],
who were the first to map the input sentence into a vector and then
back to a sentence, although they map sentences to vectors using
convolutional neural networks, which lose the ordering of the words.
Similarly to this work, Cho et al.~[cho14] used an LSTM-like RNN
architecture to map sentences into vectors and back, although their
primary focus was on integrating their neural network into an SMT
system.  Bahdanau et al.~[bog14] also attempted direct
translations with a neural network that used an attention mechanism to
overcome the poor performance on long sentences experienced by Cho et
al.~[cho14] and achieved encouraging results.  Likewise,
Pouget-Abadie et al.~[curse] attempted to address the memory
problem of Cho et al.~[cho14] by translating pieces of the source
sentence in way that produces smooth translations, which is similar to
a phrase-based approach.  We suspect that they could achieve similar
improvements by simply training their networks on reversed source
sentences.</p>

<p>End-to-end training is also the focus of Hermann et
al.~[hermann14], whose model represents the inputs and outputs by
feedforward networks, and map them to similar points in
space. However, their approach cannot generate translations directly:
to get a translation, they need to do a look up for closest vector in
the pre-computed database of sentences, or to rescore a sentence.</p>

<hr>

<h2>5. Conclusion</h2>

<p>In this work, we showed that a large deep LSTM, that has a limited 
vocabulary and that makes almost no
assumption about problem structure can outperform a standard SMT-based system whose vocabulary
is unlimited on a large-scale MT task.  The success of our simple
LSTM-based approach on MT suggests that it should do well on many
other sequence learning problems, provided they have enough training
data.</p>

<p>We were surprised by the extent of the improvement obtained by
reversing the words in the source sentences.  We conclude that it is
important to find a problem encoding that has the greatest number of
short term dependencies, as they make the learning problem much
simpler.  In particular, while we were unable to train a standard
RNN on the non-reversed translation problem (shown in
fig.~<a href="#fig:translation-model2" class="cross-ref">Fig. 1</a>), we believe that a standard RNN
should be easily trainable when the source sentences are reversed (although we
did not verify it experimentally).</p>

<p>We were also surprised by the ability of the LSTM to correctly
translate very long sentences.  We were initially convinced that the
LSTM would fail on long sentences due to its limited memory, and other
researchers reported poor performance on long sentences with a model
similar to ours [cho14][bog14][curse].  And yet,
LSTMs trained on the reversed dataset had little difficulty translating long
sentences.</p>

<p>Most importantly, we demonstrated that a simple, straightforward and a
relatively unoptimized approach can outperform an SMT system, so
further work will likely lead to even greater translation accuracies.  
These results suggest that our approach will likely   
do well on other challenging sequence to sequence problems.</p>

<p>\small</p>

<hr>

<h2>6. Acknowledgments</h2>

<p>We thank Samy Bengio, Jeff Dean, Matthieu Devin, Geoffrey Hinton, Nal Kalchbrenner, Thang Luong, Wolfgang
Macherey, Rajat Monga, Vincent Vanhoucke, Peng Xu, Wojciech Zaremba,
and the Google Brain team for useful comments and discussions.</p>

<p>\bibliography{translate} 
\bibliographystyle{plain}</p>

<hr>

<h2>References</h2>
<ol class="references">
</ol>
