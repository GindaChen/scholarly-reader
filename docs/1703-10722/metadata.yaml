title: Factorization tricks for LSTM networks
short_title: Factorization tricks for LSTM networks
type: journal-article
authors: []
date: '2026-03-02'
url: https://arxiv.org/abs/1703.10722
pdf: https://arxiv.org/pdf/1703.10722
arxiv_id: '1703.10722'
archive: arxiv
archive_url: https://arxiv.org/abs/1703.10722
source: arxiv-pipeline
pipeline_version: '2.0'
tags:
  - auto-imported
  - arxiv-pipeline
abstract: >-
  We present two simple ways of reducing the number of parameters and accelerating the training of
  large Long Short-Term Memory (LSTM) networks: the first one is "matrix factorization by design" of
  LSTM matrix into the product of two smaller matrices, and the second one is partitioning of LSTM
  matrix, its inputs and states into the independent groups. Both approaches allow us to train large
  LSTM networks significantly faster to the near state-of the art perplexity while using
  significantly less RN
files:
  - name: paper.html
    format: html
    description: Rendered with KaTeX math and extracted figures
    primary: true
variable_count: 0
equation_count: 4
reference_count: 0
sections: 4
figures: 2
