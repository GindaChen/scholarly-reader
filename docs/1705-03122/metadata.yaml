title: Convolutional Sequence to Sequence Learning
short_title: Convolutional Sequence to Sequence Learnâ€¦
type: journal-article
authors:
  - Jonas Gehring
  - Michael Auli
  - David Grangier
  - Denis Yarats
  - Yann N. Dauphin
date: '2026-03-02'
url: https://arxiv.org/abs/1705.03122
pdf: https://arxiv.org/pdf/1705.03122
arxiv_id: '1705.03122'
archive: arxiv
archive_url: https://arxiv.org/abs/1705.03122
source: arxiv-pipeline
pipeline_version: '2.2'
tags:
  - auto-imported
  - arxiv-pipeline
  - pdf-only
abstract: >-
  The prevalent approach to sequence to sequence learning maps an input sequence
  to a variable length output sequence via recurrent neural networks. We
  introduce an architecture based entirely on convolutional neural networks.
  Compared to recurrent models, computations over all elements can be fully
  parallelized during training and optimization is easier since the number of
  non-linearities is fixed and independent of the input length. Our use of gated
  linear units eases gradient propagation and we equip each decoder layer with a
  separate attention module. We outperform the accuracy of the deep LSTM setup
  of Wu et al. (2016) on both WMT'14 English-German and WMT'14 English-French
  translation at an order of magnitude faster speed, both on GPU and CPU.
files:
  - name: paper.html
    format: html
    description: PDF embed fallback
    primary: true
